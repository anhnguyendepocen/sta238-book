# Supplement to Chapter 20

This chapter implements much of the analysis shown in chapter 20 of 
A Modern Introduction to Probability and Statistics. R code is given for the
simple textbook datasets used in the book, and then the concepts are
illustrated on real data.

All datasets from the book can be downloaded here: https://www.tudelft.nl/en/eemcs/the-faculty/departments/applied-mathematics/applied-probability/education/mips/.

The assigned exercises associated with this material are from MIPS, as follows: 20.1; 20.2; 20.3; 20.4; 20.5; 20.8; 20.9; 20.11. Answers to selected exercises are in the 
back of the book. You should also do all the "quick exercises" within chapter 20 
(solutions are at the end of the chapter). Use `R` as much as possible when answering the
questions.

## Efficiency and Mean Square Error (Chapter 20)

This chapter compares estimators using the Mean Squared Error (MSE). The 
motivating example is estimating the number of German tanks using their
observed serial numbers, assuming their serial numbers are assigned uniformly
at random. Two estimators are used: one based on the sample mean, and one based
on the sample maximum.

First, let's write functions to compute these two estimators, and use simulation
to verify that they are unbiased. At this point in the course, you should start
feeling comfortable approaching this yourself. I encourage you to try this before
looking at my answer as follows:

```{r german-1,warning=FALSE,message=FALSE}
library(tidyverse)
# Functions to compute the estimators
T1 <- function(x) 2 * mean(x) - 1
T2 <- function(x) ( (length(x) + 1)/length(x) ) * max(x) - 1

# Now, simulate in order to assess their bias.
# This goes as follows (try this yourself before looking):
# - Choose a true value of N, the parameter to be estimated
# - Draw a sample of size n from 1:N without replacement
# - Compute T1 and T2
# - Repeat this M times, and compare the average of T1 and T2 to N.
N <- 1000
n <- 10
M <- 2000
# Run the simulations. Use the sample.int() function to generate from a DISCRETE
# uniform distribution
out <- 1:M %>%
  map(~sample.int(N,n)) %>%
  map(~c("T1" = T1(.x),"T2" = T2(.x))) %>%
  reduce(bind_rows)

# What do you expect the mean to be?
out %>% summarize_all(mean)
# Why does this seem to indicate that T1 and T2 have zero bias?

# Recreate the plots in Figure 20.1:
leftplot <- out %>%
  ggplot(aes(x = T1)) +
  theme_classic() + 
  geom_histogram(aes(y = ..density..),bins = 30,colour = "black",fill = "transparent") +
  scale_x_continuous(breaks = c(300,700,1000,1300,1600)) +
  coord_cartesian(xlim = c(300,1600))

rightplot <- out %>%
  ggplot(aes(x = T2)) +
  theme_classic() + 
  geom_histogram(aes(y = ..density..),bins = 30,colour = "black",fill = "transparent") +
  scale_x_continuous(breaks = c(300,700,1000,1300,1600)) +
  coord_cartesian(xlim = c(300,1600))

cowplot::plot_grid(leftplot,rightplot,nrow = 1)
```

Why does $T2$ seem to have a maximum possible value? Can you compute this mathematically?



