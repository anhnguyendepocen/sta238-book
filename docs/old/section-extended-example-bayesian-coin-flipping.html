<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 7 Extended Example: Bayesian coin flipping | STA238: Probability, Statistics, and Data Analysis</title>
  <meta name="description" content="This book represents part of the course materials for STA238 at the University of Toronto" />
  <meta name="generator" content="bookdown 0.13 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 7 Extended Example: Bayesian coin flipping | STA238: Probability, Statistics, and Data Analysis" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This book represents part of the course materials for STA238 at the University of Toronto" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 7 Extended Example: Bayesian coin flipping | STA238: Probability, Statistics, and Data Analysis" />
  
  <meta name="twitter:description" content="This book represents part of the course materials for STA238 at the University of Toronto" />
  

<meta name="author" content="Alison Gibbs and Alex Stringer" />


<meta name="date" content="2019-11-25" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="section-supplement-to-evans-rosenthal-section-7-1.html"/>
<link rel="next" href="section-supplement-to-chapter-18.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">STA238 University of Toronto</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="section-supplement-to-chapters-15-and-16.html"><a href="section-supplement-to-chapters-15-and-16.html"><i class="fa fa-check"></i><b>2</b> Supplement to Chapters 15 and 16</a><ul>
<li class="chapter" data-level="2.1" data-path="section-supplement-to-chapters-15-and-16.html"><a href="section-supplement-to-chapters-15-and-16.html#section-old-faithful"><i class="fa fa-check"></i><b>2.1</b> Old Faithful</a><ul>
<li class="chapter" data-level="2.1.1" data-path="section-supplement-to-chapters-15-and-16.html"><a href="section-supplement-to-chapters-15-and-16.html#section-read-in-the-data"><i class="fa fa-check"></i><b>2.1.1</b> Read in the data</a></li>
<li class="chapter" data-level="2.1.2" data-path="section-supplement-to-chapters-15-and-16.html"><a href="section-supplement-to-chapters-15-and-16.html#section-graphical-summaries"><i class="fa fa-check"></i><b>2.1.2</b> Graphical Summaries</a></li>
<li class="chapter" data-level="2.1.3" data-path="section-supplement-to-chapters-15-and-16.html"><a href="section-supplement-to-chapters-15-and-16.html#section-numerical-summaries-ch-16"><i class="fa fa-check"></i><b>2.1.3</b> Numerical Summaries (Ch 16)</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="section-supplement-to-chapters-15-and-16.html"><a href="section-supplement-to-chapters-15-and-16.html#section-drilling"><i class="fa fa-check"></i><b>2.2</b> Drilling</a><ul>
<li class="chapter" data-level="2.2.1" data-path="section-supplement-to-chapters-15-and-16.html"><a href="section-supplement-to-chapters-15-and-16.html#section-read-in-data"><i class="fa fa-check"></i><b>2.2.1</b> Read in data</a></li>
<li class="chapter" data-level="2.2.2" data-path="section-supplement-to-chapters-15-and-16.html"><a href="section-supplement-to-chapters-15-and-16.html#section-graphical-summaries-1"><i class="fa fa-check"></i><b>2.2.2</b> Graphical summaries</a></li>
<li class="chapter" data-level="2.2.3" data-path="section-supplement-to-chapters-15-and-16.html"><a href="section-supplement-to-chapters-15-and-16.html#section-numerical-summaries"><i class="fa fa-check"></i><b>2.2.3</b> Numerical summaries</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="section-supplement-to-chapters-15-and-16.html"><a href="section-supplement-to-chapters-15-and-16.html#section-exercises"><i class="fa fa-check"></i><b>2.3</b> Exercises</a></li>
<li class="chapter" data-level="2.4" data-path="section-supplement-to-chapters-15-and-16.html"><a href="section-supplement-to-chapters-15-and-16.html#section-extended-example-smoking-and-age-and-mortality"><i class="fa fa-check"></i><b>2.4</b> Extended example: smoking and age and mortality</a><ul>
<li class="chapter" data-level="2.4.1" data-path="section-supplement-to-chapters-15-and-16.html"><a href="section-supplement-to-chapters-15-and-16.html#section-exercises-1"><i class="fa fa-check"></i><b>2.4.1</b> Exercises</a></li>
<li class="chapter" data-level="2.4.2" data-path="section-supplement-to-chapters-15-and-16.html"><a href="section-supplement-to-chapters-15-and-16.html#section-exercises-2"><i class="fa fa-check"></i><b>2.4.2</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="section-supplement-to-chapters-15-and-16.html"><a href="section-supplement-to-chapters-15-and-16.html#section-case-study-rental-housing-in-toronto"><i class="fa fa-check"></i><b>2.5</b> Case study: rental housing in Toronto</a><ul>
<li class="chapter" data-level="2.5.1" data-path="section-supplement-to-chapters-15-and-16.html"><a href="section-supplement-to-chapters-15-and-16.html#section-load-the-data"><i class="fa fa-check"></i><b>2.5.1</b> Load the data</a></li>
<li class="chapter" data-level="2.5.2" data-path="section-supplement-to-chapters-15-and-16.html"><a href="section-supplement-to-chapters-15-and-16.html#section-analysis-i-what-does-the-data-look-like"><i class="fa fa-check"></i><b>2.5.2</b> Analysis I: what does the data look like?</a></li>
<li class="chapter" data-level="2.5.3" data-path="section-supplement-to-chapters-15-and-16.html"><a href="section-supplement-to-chapters-15-and-16.html#section-analysis-ii-do-different-wards-have-different-quality-housing"><i class="fa fa-check"></i><b>2.5.3</b> Analysis II: Do different wards have different quality housing?</a></li>
<li class="chapter" data-level="2.5.4" data-path="section-supplement-to-chapters-15-and-16.html"><a href="section-supplement-to-chapters-15-and-16.html#section-analysis-iii-trends-in-quality-over-time"><i class="fa fa-check"></i><b>2.5.4</b> Analysis III: trends in quality over time</a></li>
<li class="chapter" data-level="2.5.5" data-path="section-supplement-to-chapters-15-and-16.html"><a href="section-supplement-to-chapters-15-and-16.html#section-summary"><i class="fa fa-check"></i><b>2.5.5</b> Summary</a></li>
<li class="chapter" data-level="2.5.6" data-path="section-supplement-to-chapters-15-and-16.html"><a href="section-supplement-to-chapters-15-and-16.html#section-exercises-3"><i class="fa fa-check"></i><b>2.5.6</b> Exercises</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="section-supplement-to-chapters-13-and-14.html"><a href="section-supplement-to-chapters-13-and-14.html"><i class="fa fa-check"></i><b>3</b> Supplement to Chapters 13 and 14</a><ul>
<li class="chapter" data-level="3.1" data-path="section-supplement-to-chapters-13-and-14.html"><a href="section-supplement-to-chapters-13-and-14.html#section-law-of-large-numbers-chapter-13"><i class="fa fa-check"></i><b>3.1</b> Law of Large Numbers (Chapter 13)</a><ul>
<li class="chapter" data-level="3.1.1" data-path="section-supplement-to-chapters-13-and-14.html"><a href="section-supplement-to-chapters-13-and-14.html#section-extended-example-the-probability-of-heads"><i class="fa fa-check"></i><b>3.1.1</b> Extended example: the probability of heads</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="section-supplement-to-chapters-13-and-14.html"><a href="section-supplement-to-chapters-13-and-14.html#section-central-limit-theorem-chapter-14"><i class="fa fa-check"></i><b>3.2</b> Central Limit Theorem (Chapter 14)</a><ul>
<li class="chapter" data-level="3.2.1" data-path="section-supplement-to-chapters-13-and-14.html"><a href="section-supplement-to-chapters-13-and-14.html#section-extended-example-the-probability-of-heads-1"><i class="fa fa-check"></i><b>3.2.1</b> Extended example: the probability of heads</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="section-supplement-to-chapters-17-and-19.html"><a href="section-supplement-to-chapters-17-and-19.html"><i class="fa fa-check"></i><b>4</b> Supplement to Chapters 17 and 19</a><ul>
<li class="chapter" data-level="4.1" data-path="section-supplement-to-chapters-17-and-19.html"><a href="section-supplement-to-chapters-17-and-19.html#section-statistical-models-chapter-17"><i class="fa fa-check"></i><b>4.1</b> Statistical models (Chapter 17)</a><ul>
<li class="chapter" data-level="4.1.1" data-path="section-supplement-to-chapters-17-and-19.html"><a href="section-supplement-to-chapters-17-and-19.html#section-janka-hardness-data"><i class="fa fa-check"></i><b>4.1.1</b> Janka Hardness data</a></li>
<li class="chapter" data-level="4.1.2" data-path="section-supplement-to-chapters-17-and-19.html"><a href="section-supplement-to-chapters-17-and-19.html#section-extended-example-ttc-ridership-revenues"><i class="fa fa-check"></i><b>4.1.2</b> Extended example: TTC ridership revenues</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="section-supplement-to-chapters-17-and-19.html"><a href="section-supplement-to-chapters-17-and-19.html#section-unbiased-estimators-chapter-19"><i class="fa fa-check"></i><b>4.2</b> Unbiased Estimators (Chapter 19)</a><ul>
<li class="chapter" data-level="4.2.1" data-path="section-supplement-to-chapters-17-and-19.html"><a href="section-supplement-to-chapters-17-and-19.html#section-simulated-network-data"><i class="fa fa-check"></i><b>4.2.1</b> Simulated network data</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="section-supplement-to-chapter-20.html"><a href="section-supplement-to-chapter-20.html"><i class="fa fa-check"></i><b>5</b> Supplement to Chapter 20</a><ul>
<li class="chapter" data-level="5.1" data-path="section-supplement-to-chapter-20.html"><a href="section-supplement-to-chapter-20.html#section-efficiency-and-mean-square-error-chapter-20"><i class="fa fa-check"></i><b>5.1</b> Efficiency and Mean Square Error (Chapter 20)</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="section-supplement-to-evans-rosenthal-section-7-1.html"><a href="section-supplement-to-evans-rosenthal-section-7-1.html"><i class="fa fa-check"></i><b>6</b> Supplement to Evans &amp; Rosenthal Section 7.1</a></li>
<li class="chapter" data-level="7" data-path="section-extended-example-bayesian-coin-flipping.html"><a href="section-extended-example-bayesian-coin-flipping.html"><i class="fa fa-check"></i><b>7</b> Extended Example: Bayesian coin flipping</a><ul>
<li class="chapter" data-level="7.1" data-path="section-extended-example-bayesian-coin-flipping.html"><a href="section-extended-example-bayesian-coin-flipping.html#section-tutorial"><i class="fa fa-check"></i><b>7.1</b> Tutorial</a><ul>
<li class="chapter" data-level="7.1.1" data-path="section-extended-example-bayesian-coin-flipping.html"><a href="section-extended-example-bayesian-coin-flipping.html#section-frequentistlikelihood-perspective"><i class="fa fa-check"></i><b>7.1.1</b> Frequentist/Likelihood Perspective</a></li>
<li class="chapter" data-level="7.1.2" data-path="section-extended-example-bayesian-coin-flipping.html"><a href="section-extended-example-bayesian-coin-flipping.html#section-bayesian-inference-introduction"><i class="fa fa-check"></i><b>7.1.2</b> Bayesian Inference: introduction</a></li>
<li class="chapter" data-level="7.1.3" data-path="section-extended-example-bayesian-coin-flipping.html"><a href="section-extended-example-bayesian-coin-flipping.html#section-flipping-more-coins"><i class="fa fa-check"></i><b>7.1.3</b> Flipping More Coins</a></li>
<li class="chapter" data-level="7.1.4" data-path="section-extended-example-bayesian-coin-flipping.html"><a href="section-extended-example-bayesian-coin-flipping.html#section-visualization"><i class="fa fa-check"></i><b>7.1.4</b> Visualization</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="section-extended-example-bayesian-coin-flipping.html"><a href="section-extended-example-bayesian-coin-flipping.html#section-interactive-app"><i class="fa fa-check"></i><b>7.2</b> Interactive App</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="section-supplement-to-chapter-18.html"><a href="section-supplement-to-chapter-18.html"><i class="fa fa-check"></i><b>8</b> Supplement to Chapter 18</a><ul>
<li class="chapter" data-level="8.1" data-path="section-supplement-to-chapter-18.html"><a href="section-supplement-to-chapter-18.html#section-the-bootstrap-chapter-18"><i class="fa fa-check"></i><b>8.1</b> The Bootstrap (Chapter 18)</a><ul>
<li class="chapter" data-level="8.1.1" data-path="section-supplement-to-chapter-18.html"><a href="section-supplement-to-chapter-18.html#section-empirical-bootstrap-old-faithful-data"><i class="fa fa-check"></i><b>8.1.1</b> Empirical bootstrap: Old Faithful data</a></li>
<li class="chapter" data-level="8.1.2" data-path="section-supplement-to-chapter-18.html"><a href="section-supplement-to-chapter-18.html#section-parametric-bootstrap-software-data"><i class="fa fa-check"></i><b>8.1.2</b> Parametric Bootstrap: software data</a></li>
<li class="chapter" data-level="8.1.3" data-path="section-supplement-to-chapter-18.html"><a href="section-supplement-to-chapter-18.html#section-extended-example-the-standard-error-of-a-proportion"><i class="fa fa-check"></i><b>8.1.3</b> Extended example: the standard error of a proportion</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="section-supplement-to-chapter-21.html"><a href="section-supplement-to-chapter-21.html"><i class="fa fa-check"></i><b>9</b> Supplement to Chapter 21</a><ul>
<li class="chapter" data-level="9.1" data-path="section-supplement-to-chapter-21.html"><a href="section-supplement-to-chapter-21.html#section-maximum-likelihood-chapter-21"><i class="fa fa-check"></i><b>9.1</b> Maximum Likelihood (Chapter 21)</a><ul>
<li class="chapter" data-level="9.1.1" data-path="section-supplement-to-chapter-21.html"><a href="section-supplement-to-chapter-21.html#section-example-two-coins"><i class="fa fa-check"></i><b>9.1.1</b> Example: two coins</a></li>
<li class="chapter" data-level="9.1.2" data-path="section-supplement-to-chapter-21.html"><a href="section-supplement-to-chapter-21.html#section-example-unknown-coins-n-2"><i class="fa fa-check"></i><b>9.1.2</b> Example: unknown coins, <span class="math inline">\(n = 2\)</span></a></li>
<li class="chapter" data-level="9.1.3" data-path="section-supplement-to-chapter-21.html"><a href="section-supplement-to-chapter-21.html#section-example-unknown-coins-n-bigger-than-2"><i class="fa fa-check"></i><b>9.1.3</b> Example: unknown coins, <span class="math inline">\(n\)</span> bigger than <span class="math inline">\(2\)</span></a></li>
<li class="chapter" data-level="9.1.4" data-path="section-supplement-to-chapter-21.html"><a href="section-supplement-to-chapter-21.html#section-extended-example-rental-housing-in-toronto"><i class="fa fa-check"></i><b>9.1.4</b> Extended example: rental housing in Toronto</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="section-supplement-to-chapter-23-and-24.html"><a href="section-supplement-to-chapter-23-and-24.html"><i class="fa fa-check"></i><b>10</b> Supplement to Chapter 23 and 24</a><ul>
<li class="chapter" data-level="10.1" data-path="section-supplement-to-chapter-23-and-24.html"><a href="section-supplement-to-chapter-23-and-24.html#section-confidence-intervals-for-the-mean-chapter-23"><i class="fa fa-check"></i><b>10.1</b> Confidence Intervals for the Mean (Chapter 23)</a><ul>
<li class="chapter" data-level="10.1.1" data-path="section-supplement-to-chapter-23-and-24.html"><a href="section-supplement-to-chapter-23-and-24.html#section-simulation"><i class="fa fa-check"></i><b>10.1.1</b> Simulation</a></li>
<li class="chapter" data-level="10.1.2" data-path="section-supplement-to-chapter-23-and-24.html"><a href="section-supplement-to-chapter-23-and-24.html#section-gross-calorific-value-measurements-for-osterfeld-262de27"><i class="fa fa-check"></i><b>10.1.2</b> Gross calorific value measurements for Osterfeld 262DE27</a></li>
<li class="chapter" data-level="10.1.3" data-path="section-supplement-to-chapter-23-and-24.html"><a href="section-supplement-to-chapter-23-and-24.html#section-gross-calorific-value-measurements-for-daw-mill-258gb41"><i class="fa fa-check"></i><b>10.1.3</b> Gross calorific value measurements for Daw Mill 258GB41</a></li>
<li class="chapter" data-level="10.1.4" data-path="section-supplement-to-chapter-23-and-24.html"><a href="section-supplement-to-chapter-23-and-24.html#section-bootstrap-confidence-intervals"><i class="fa fa-check"></i><b>10.1.4</b> Bootstrap Confidence Intervals</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="section-supplement-to-chapter-23-and-24.html"><a href="section-supplement-to-chapter-23-and-24.html#section-more-on-confidence-intervals-chapter-24"><i class="fa fa-check"></i><b>10.2</b> More on confidence intervals (Chapter 24)</a><ul>
<li class="chapter" data-level="10.2.1" data-path="section-supplement-to-chapter-23-and-24.html"><a href="section-supplement-to-chapter-23-and-24.html#section-binomial-distribution"><i class="fa fa-check"></i><b>10.2.1</b> Binomial distribution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="section-extended-example-reasoning-about-goodness-of-fit.html"><a href="section-extended-example-reasoning-about-goodness-of-fit.html"><i class="fa fa-check"></i><b>11</b> Extended Example: Reasoning About Goodness of Fit</a><ul>
<li class="chapter" data-level="11.1" data-path="section-extended-example-reasoning-about-goodness-of-fit.html"><a href="section-extended-example-reasoning-about-goodness-of-fit.html#section-go-and-read-the-blog-post"><i class="fa fa-check"></i><b>11.1</b> Go and read the blog post</a></li>
<li class="chapter" data-level="11.2" data-path="section-extended-example-reasoning-about-goodness-of-fit.html"><a href="section-extended-example-reasoning-about-goodness-of-fit.html#section-distribution-of-last-digits"><i class="fa fa-check"></i><b>11.2</b> Distribution of last digits</a><ul>
<li class="chapter" data-level="11.2.1" data-path="section-extended-example-reasoning-about-goodness-of-fit.html"><a href="section-extended-example-reasoning-about-goodness-of-fit.html#section-read-in-the-data-1"><i class="fa fa-check"></i><b>11.2.1</b> Read in the data</a></li>
<li class="chapter" data-level="11.2.2" data-path="section-extended-example-reasoning-about-goodness-of-fit.html"><a href="section-extended-example-reasoning-about-goodness-of-fit.html#section-make-the-histogram"><i class="fa fa-check"></i><b>11.2.2</b> Make the histogram</a></li>
<li class="chapter" data-level="11.2.3" data-path="section-extended-example-reasoning-about-goodness-of-fit.html"><a href="section-extended-example-reasoning-about-goodness-of-fit.html#section-testing-goodness-of-fit-simulation"><i class="fa fa-check"></i><b>11.2.3</b> Testing goodness of fit: simulation</a></li>
<li class="chapter" data-level="11.2.4" data-path="section-extended-example-reasoning-about-goodness-of-fit.html"><a href="section-extended-example-reasoning-about-goodness-of-fit.html#section-testing-goodness-of-fit-math"><i class="fa fa-check"></i><b>11.2.4</b> Testing goodness of fit: math</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">STA238: Probability, Statistics, and Data Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="section-extended-example-bayesian-coin-flipping" class="section level1">
<h1><span class="header-section-number">Chapter 7</span> Extended Example: Bayesian coin flipping</h1>
<p>In this example we will discuss at length the Beta-Bernoulli example
from section 7.1. First follow along with the tutorial, then check out the interactive app.</p>
<div id="section-tutorial" class="section level2">
<h2><span class="header-section-number">7.1</span> Tutorial</h2>
<p>Given data generated from some family of probability distributions indexed by unknown parameter, statistical inference is concerned with <strong>estimating</strong> these parameters- finding reasonable values for them, given the observed data. The central notion is that of <em>uncertainty</em>: we simply don’t know the values of the parameters that generated the data we observed, and we do know that several different values could reasonably have generated these data. <strong>Probability</strong> is the mathematical construct used to represent uncertainty.</p>
<div id="section-frequentistlikelihood-perspective" class="section level3">
<h3><span class="header-section-number">7.1.1</span> Frequentist/Likelihood Perspective</h3>
<p>Classically, the approach to this problem is taught from the <strong>frequentist perspective</strong>. Uncertainty in the values of the parameters that generated the data is represented by probability via the notion of <em>repeated sampling</em>: under the given probability model with the given parameter values, what is the relative frequency with which these same data would be observed, if the experiment that generated the data were repeated again and again? Values of the parameters that have a higher probability of having generated the observed data are thought to be more likely than other values.</p>
<p>As an example, consider a coin with unknown probability of heads <span class="math inline">\(\theta\)</span>. We toss the coin once, and observe random outcome (data) <span class="math inline">\(X = 1\)</span> if the toss is heads and <span class="math inline">\(X = 0\)</span> if not. For simplicity, suppose we know we have chosen one of two possible coins, either having <span class="math inline">\(\theta = 0.7\)</span> or <span class="math inline">\(\theta = 0.3\)</span>. How do we use the observed data to infer which of these two coins we threw?</p>
<p>For any <span class="math inline">\(0 &lt; \theta &lt; 1\)</span>, the probability distribution of the single coin toss is given by
<span class="math display">\[
P(X = x) = \theta^{x}(1-\theta)^{1-x}
\]</span>
This just says that <span class="math inline">\(P(X = 1) = \theta\)</span> and <span class="math inline">\(P(X = 0) = 1-\theta\)</span>.</p>
<p>Let’s say we throw the coin once, and observe <span class="math inline">\(X = 1\)</span>. If <span class="math inline">\(\theta = 0.7\)</span> the probabilty of observing this result is <span class="math inline">\(P(X = 1|\theta = 0.7) = 0.7\)</span>. That is if <span class="math inline">\(\theta = 0.7\)</span>, we would expect roughly <span class="math inline">\(70\%\)</span> of repetitions of this experiment to yield the same results as we observed in our data. If <span class="math inline">\(\theta = 0.3\)</span> on the other hand, <span class="math inline">\(P(X = 1|\theta = 0.3) = 0.3\)</span>; only <span class="math inline">\(30\%\)</span> of the repetitions of this experiment would yield the observed data if <span class="math inline">\(\theta = 0.3\)</span>. Because <span class="math inline">\(\theta = 0.7\)</span> would yield the observed data more frequently than <span class="math inline">\(\theta = 0.3\)</span>, we say that <span class="math inline">\(\theta = 0.7\)</span> is <em>more likely</em> to have generated the observed data than <span class="math inline">\(\theta=0.3\)</span>, and our inference favours <span class="math inline">\(\theta =0.7\)</span>.</p>
</div>
<div id="section-bayesian-inference-introduction" class="section level3">
<h3><span class="header-section-number">7.1.2</span> Bayesian Inference: introduction</h3>
<p>One criticism of the above approach is that is depends not only on the observed data, but also on infinitely many other possible datasets that are not observed. This is an artifact of the manner in which probability is used to represent uncertainty. In contrast, Bayesian statistics represents uncertainty about the value of a parameter <strong>directly</strong> using probability distributions.</p>
<p>In particular, a <strong>prior distribution</strong> is placed on the parameter, representing the probable values of that parameter before data is observed. Having observed the data, the prior is updated via <em>Bayes’ Rule</em>, yielding the <strong>posterior distribution</strong> of the parameter, given the data.</p>
<p>The choice of prior distribution is based either on subject-matter knowledge or mathematical convenience, and is a subjective choice on the part of the analyst. Don’t worry too much about it in this course– you should get comfortable with the idea and the math. We’d talk more about it in a more advanced applied statistics course.</p>
<p>To see how this works, suppose that we think about <span class="math inline">\(4/5\)</span> coins in our pocket are the <span class="math inline">\(\theta = 0.3\)</span> coins, and only <span class="math inline">\(1/5\)</span> are the <span class="math inline">\(\theta = 0.7\)</span> coins. The parameter space here is <span class="math inline">\(\Theta = \left\{ 0.3, 0.7\right\}\)</span>. Our prior distribution on <span class="math inline">\(\theta\)</span> is then
<span class="math display">\[
P(\theta = q) = 0.2^{I(q = 0.7)}0.8^{I(q = 0.3)}, \ q\in\Theta
\]</span>
where <span class="math inline">\(I(q = 0.7) = 1\)</span> if <span class="math inline">\(q = 0.7\)</span> and <span class="math inline">\(0\)</span> otherwise. This, like the probability distribution of the actual result of the coin toss, just encodes our notion that <span class="math inline">\(P(\theta = 0.3) = 0.8\)</span> and <span class="math inline">\(P(\theta = 0.7) = 0.2\)</span>.</p>
<p>So without knowing the result of the coin toss, we think there is a <span class="math inline">\(20\%\)</span> chance that <span class="math inline">\(\theta = 0.7\)</span>. We know from above that if we observe heads on the coin toss, we have observed a result that would occur about <span class="math inline">\(70\%\)</span> of the time if <span class="math inline">\(\theta = 0.7\)</span>. In probability terms, we have a <em>marginal</em> distribution for <span class="math inline">\(\theta\)</span>, and a <em>conditional</em> distribution for <span class="math inline">\(X|\theta\)</span>.</p>
<p>These two ideas are combined by computing the conditional distribution of <span class="math inline">\(\theta|X\)</span>, known as the <strong>posterior distribution</strong> for <span class="math inline">\(\theta\)</span> having observed <span class="math inline">\(X\)</span>. This is obtained (explaining the name) via <em>Bayes’ Rule</em>:
<span class="math display">\[
p(\theta|X) = \frac{p(X|\theta)\times p(\theta)}{p(X)}
\]</span>
where the marginal distribution of <span class="math inline">\(X\)</span>, or the <em>normalizing constant</em> or <em>marginal likelihood</em> or <em>model evidence</em> (this thing has a lot of names) is given by
<span class="math display">\[
p(X) = \sum_{\theta\in\Theta}p(X|\theta)\times p(\theta)
\]</span>
and ensures <span class="math inline">\(p(\theta|X)\)</span> is a proper probability distribution.</p>
<p><strong>Exercise</strong>: verify that <span class="math inline">\(p(\theta|X)\)</span> is a valid probability density on <span class="math inline">\(\Theta\)</span> by verifying that <span class="math inline">\(\sum_{\theta\in\Theta}p(\theta|X) = 1\)</span>.</p>
<p>In our example, the prior probability of <span class="math inline">\(\theta = 0.7\)</span> is only <span class="math inline">\(20\%\)</span>. But we flip the coin and observe <span class="math inline">\(X = 1\)</span>. We can see how this observation updates our belief about the likely values of <span class="math inline">\(\theta\)</span> by computing the posterior distribution of <span class="math inline">\(\theta\)</span> given the observed data:
<span class="math display">\[
\begin{aligned}
&amp;p(\theta|X) = \frac{\theta^{x}(1-\theta)^{1-x}\times 0.2^{I(\theta = 0.7)}0.8^{I(\theta = 0.3)}}{\sum_{\theta = 0.3,0.7}\theta^{x}(1-\theta)^{1-x}\times 0.2^{I(\theta = 0.7)}0.8^{I(\theta = 0.3)}} \\
\implies&amp; P(\theta = 0.7 | X = 1) = \frac{0.7 \times 0.2}{0.7\times0.2 + 0.3\times0.8} \\
&amp;= 0.368
\end{aligned}
\]</span>
Before observing heads, we would have thought the <span class="math inline">\(\theta = 0.7\)</span> coin to be very unlikely, but because the observed data favours <span class="math inline">\(\theta = 0.7\)</span> more strongly than <span class="math inline">\(\theta = 0.3\)</span>, after observing these data we feel that <span class="math inline">\(\theta = 0.7\)</span> is more likely than before.</p>
</div>
<div id="section-flipping-more-coins" class="section level3">
<h3><span class="header-section-number">7.1.3</span> Flipping More Coins</h3>
<p>Suppose now that we flip <span class="math inline">\(n\)</span> coins, obtaining a dataset <span class="math inline">\(X = (X_{1},\ldots,X_{n})\)</span> of heads or tails, represented by 0’s and 1’s. If we’re still considering only two candidate values <span class="math inline">\(\theta = 0.7\)</span> or <span class="math inline">\(\theta = 0.3\)</span>, we may still ask the question “which value is more likely to have generated the observed data?”. We again form the <em>likelihood function</em> for each value of <span class="math inline">\(\theta\)</span>, the relative frequency with which each value of <span class="math inline">\(\theta\)</span> would have generated the observed sample. Assuming the tosses are statistically independent:
<span class="math display">\[
p(X|\theta) = \theta^{\sum_{i=1}^{n}X_{i}} \times (1 - \theta)^{n - \sum_{i=1}^{n}X_{i}}
\]</span>
where <span class="math inline">\(\sum_{i=1}^{n}X_{i}\)</span> is just the number of heads observed in the sample. We see that any two samples that have the same number of heads will lead to the same inferences about <span class="math inline">\(\theta\)</span> in this manner.</p>
<p>Suppose we throw the coin <span class="math inline">\(10\)</span> times and observe <span class="math inline">\(6\)</span> heads. The likelihood function for each candidate value of <span class="math inline">\(\theta\)</span> is
<span class="math display">\[
\begin{aligned}
p(X|\theta = 0.7) &amp;= 0.7^{6} \times 0.3^{4} = 0.000953 \\
p(X|\theta = 0.3) &amp;= 0.3^{6} \times 0.7^{4} = 0.000175 \\
\end{aligned}
\]</span>
It is much more likely to observe <span class="math inline">\(6\)</span> heads when <span class="math inline">\(\theta = 0.7\)</span> than when <span class="math inline">\(\theta = 0.3\)</span>.</p>
<p><strong>Exercise</strong>: calculate the likelihood function for <span class="math inline">\(\theta\)</span> when <span class="math inline">\(n = 100\)</span> and <span class="math inline">\(\sum_{i=1}^{n}X_{i} = 60\)</span>. How much more likely is <span class="math inline">\(\theta = 0.7\)</span> than <span class="math inline">\(\theta = 0.3\)</span> now?</p>
<p>In the Bayesian setting, with our prior distribution on <span class="math inline">\(\theta\)</span> from above, we would form the posterior distribution as follows:
<span class="math display">\[
p(\theta|X) = \frac{\theta^{\sum_{i=1}^{n}x_{i}}(1-\theta)^{n - \sum_{i=1}^{n}x_{i}}\times 0.2^{I(\theta = 0.7)}0.8^{I(\theta = 0.3)}}{\sum_{\theta = 0.3,0.7}\theta^{\sum_{i=1}^{n}x_{i}}(1-\theta)^{n - \sum_{i=1}^{n}x_{i}}\times 0.2^{I(\theta = 0.7)}0.8^{I(\theta = 0.3)}}
\]</span>
Computing this for our observed data of <span class="math inline">\(\sum_{i=1}^{10}x_{i} = 6\)</span> yields
<span class="math display">\[
\begin{aligned}
p(\theta = 0.7 |X) = \frac{0.7^{6}0.3^{4}\times 0.2}{0.7^{6}0.3^{4}\times 0.2 + 0.3^{6}0.7^4\times0.8} = 0.576 \\
p(\theta = 0.3 |X) = \frac{0.3^{6}0.7^{4}\times 0.2}{0.3^{6}0.7^{4}\times 0.8 + 0.7^{6}0.3^4\times0.2} = 0.424 \\
\end{aligned}
\]</span></p>
<p>We can see that the data “updates” our prior belief that <span class="math inline">\(\theta = 0.3\)</span> was more probable than <span class="math inline">\(\theta = 0.7\)</span>, because the observed data was more likely to have occurred if <span class="math inline">\(\theta = 0.7\)</span> than if <span class="math inline">\(\theta = 0.3\)</span>.</p>
</div>
<div id="section-visualization" class="section level3">
<h3><span class="header-section-number">7.1.4</span> Visualization</h3>
<p>It is helpful to visualize the prior and posterior, for the observed data. Because both prior and posterior only allow two values, we can do this using a simple bar chart:</p>
<div class="sourceCode" id="section-cb2"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2-1" title="1">visualize_binomial_priorposterior &lt;-<span class="st"> </span><span class="cf">function</span>(sumx,n) {</a>
<a class="sourceLine" id="cb2-2" title="2">  prior &lt;-<span class="st"> </span><span class="cf">function</span>(theta) {</a>
<a class="sourceLine" id="cb2-3" title="3">    <span class="cf">if</span> (theta <span class="op">==</span><span class="st"> </span><span class="fl">.3</span>) {</a>
<a class="sourceLine" id="cb2-4" title="4">      <span class="kw">return</span>(.<span class="dv">8</span>)</a>
<a class="sourceLine" id="cb2-5" title="5">    }</a>
<a class="sourceLine" id="cb2-6" title="6">    <span class="cf">else</span> <span class="cf">if</span> (theta <span class="op">==</span><span class="st"> </span><span class="fl">.7</span>) {</a>
<a class="sourceLine" id="cb2-7" title="7">      <span class="kw">return</span>(.<span class="dv">2</span>)</a>
<a class="sourceLine" id="cb2-8" title="8">    }</a>
<a class="sourceLine" id="cb2-9" title="9">    <span class="dv">0</span></a>
<a class="sourceLine" id="cb2-10" title="10">  }</a>
<a class="sourceLine" id="cb2-11" title="11">  likelihood &lt;-<span class="st"> </span><span class="cf">function</span>(theta) theta<span class="op">^</span>sumx <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span><span class="op">-</span>theta)<span class="op">^</span>(n <span class="op">-</span><span class="st"> </span>sumx)</a>
<a class="sourceLine" id="cb2-12" title="12">  marginal_likelihood &lt;-<span class="st"> </span><span class="kw">prior</span>(.<span class="dv">7</span>) <span class="op">*</span><span class="st"> </span><span class="kw">likelihood</span>(.<span class="dv">7</span>) <span class="op">+</span><span class="st"> </span><span class="kw">prior</span>(.<span class="dv">3</span>) <span class="op">*</span><span class="st"> </span><span class="kw">likelihood</span>(.<span class="dv">3</span>)</a>
<a class="sourceLine" id="cb2-13" title="13">  posterior &lt;-<span class="st"> </span><span class="cf">function</span>(theta) <span class="kw">likelihood</span>(theta) <span class="op">*</span><span class="st"> </span><span class="kw">prior</span>(theta) <span class="op">/</span><span class="st"> </span>marginal_likelihood</a>
<a class="sourceLine" id="cb2-14" title="14">  </a>
<a class="sourceLine" id="cb2-15" title="15">  <span class="co"># Plot of the prior and posterior distributions for these observed data</span></a>
<a class="sourceLine" id="cb2-16" title="16">  <span class="kw">data_frame</span>(</a>
<a class="sourceLine" id="cb2-17" title="17">    <span class="dt">theta =</span> <span class="kw">c</span>(.<span class="dv">3</span>,.<span class="dv">7</span>,.<span class="dv">3</span>,.<span class="dv">7</span>),</a>
<a class="sourceLine" id="cb2-18" title="18">    <span class="dt">value =</span> <span class="kw">c</span>(<span class="kw">prior</span>(.<span class="dv">3</span>),<span class="kw">prior</span>(.<span class="dv">7</span>),<span class="kw">posterior</span>(.<span class="dv">3</span>),<span class="kw">posterior</span>(.<span class="dv">7</span>)),</a>
<a class="sourceLine" id="cb2-19" title="19">    <span class="dt">type =</span> <span class="kw">c</span>(<span class="st">&quot;Prior&quot;</span>,<span class="st">&quot;Prior&quot;</span>,<span class="st">&quot;Posterior&quot;</span>,<span class="st">&quot;Posterior&quot;</span>)</a>
<a class="sourceLine" id="cb2-20" title="20">  ) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb2-21" title="21"><span class="st">    </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> theta,<span class="dt">y =</span> value,<span class="dt">fill =</span> type)) <span class="op">+</span></a>
<a class="sourceLine" id="cb2-22" title="22"><span class="st">    </span><span class="kw">theme_classic</span>() <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb2-23" title="23"><span class="st">    </span><span class="kw">geom_bar</span>(<span class="dt">stat =</span> <span class="st">&quot;identity&quot;</span>,<span class="dt">position =</span> <span class="st">&quot;dodge&quot;</span>,<span class="dt">colour =</span> <span class="st">&quot;black&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb2-24" title="24"><span class="st">    </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Prior and Posterior for theta&quot;</span>,</a>
<a class="sourceLine" id="cb2-25" title="25">         <span class="dt">subtitle =</span> <span class="kw">str_c</span>(<span class="st">&quot;Observed data: &quot;</span>,sumx,<span class="st">&quot; flips in &quot;</span>,n,<span class="st">&quot; throws&quot;</span>),</a>
<a class="sourceLine" id="cb2-26" title="26">         <span class="dt">x =</span> <span class="st">&quot;Theta, probability of heads&quot;</span>,</a>
<a class="sourceLine" id="cb2-27" title="27">         <span class="dt">y =</span> <span class="st">&quot;Prior/Posterior Probability&quot;</span>,</a>
<a class="sourceLine" id="cb2-28" title="28">         <span class="dt">fill =</span> <span class="st">&quot;&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb2-29" title="29"><span class="st">    </span><span class="kw">scale_x_continuous</span>(<span class="dt">breaks =</span> <span class="kw">c</span>(<span class="fl">0.30</span>,<span class="fl">0.70</span>),<span class="dt">labels =</span> <span class="kw">c</span>(<span class="st">&quot;0.30&quot;</span>,<span class="st">&quot;0.70&quot;</span>)) <span class="op">+</span></a>
<a class="sourceLine" id="cb2-30" title="30"><span class="st">    </span><span class="kw">scale_y_continuous</span>(<span class="dt">labels =</span> scales<span class="op">::</span><span class="kw">percent_format</span>()) <span class="op">+</span></a>
<a class="sourceLine" id="cb2-31" title="31"><span class="st">    </span><span class="kw">scale_fill_brewer</span>(<span class="dt">palette =</span> <span class="st">&quot;Reds&quot;</span>)</a>
<a class="sourceLine" id="cb2-32" title="32">  </a>
<a class="sourceLine" id="cb2-33" title="33">}</a></code></pre></div>
<p>Plotting is nice as it lets us compare how different observed data, and different experiments (number of throws) affect the prior/posterior balance of belief:</p>
<div class="sourceCode" id="section-cb3"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb3-1" title="1">cowplot<span class="op">::</span><span class="kw">plot_grid</span>(</a>
<a class="sourceLine" id="cb3-2" title="2">  <span class="kw">visualize_binomial_priorposterior</span>(<span class="dv">6</span>,<span class="dv">6</span>),</a>
<a class="sourceLine" id="cb3-3" title="3">  <span class="kw">visualize_binomial_priorposterior</span>(<span class="dv">6</span>,<span class="dv">10</span>),</a>
<a class="sourceLine" id="cb3-4" title="4">  <span class="kw">visualize_binomial_priorposterior</span>(<span class="dv">6</span>,<span class="dv">20</span>),</a>
<a class="sourceLine" id="cb3-5" title="5">  <span class="kw">visualize_binomial_priorposterior</span>(<span class="dv">6</span>,<span class="dv">50</span>),</a>
<a class="sourceLine" id="cb3-6" title="6">  <span class="kw">visualize_binomial_priorposterior</span>(<span class="dv">0</span>,<span class="dv">10</span>),</a>
<a class="sourceLine" id="cb3-7" title="7">  <span class="kw">visualize_binomial_priorposterior</span>(<span class="dv">1</span>,<span class="dv">10</span>),</a>
<a class="sourceLine" id="cb3-8" title="8">  <span class="kw">visualize_binomial_priorposterior</span>(<span class="dv">7</span>,<span class="dv">10</span>),</a>
<a class="sourceLine" id="cb3-9" title="9">  <span class="kw">visualize_binomial_priorposterior</span>(<span class="dv">10</span>,<span class="dv">10</span>),</a>
<a class="sourceLine" id="cb3-10" title="10">  <span class="dt">ncol=</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb3-11" title="11">)</a></code></pre></div>
<pre><code>## Warning: `data_frame()` is deprecated, use `tibble()`.
## This warning is displayed once per session.</code></pre>
<p><img src="book_files/figure-html/binomial-vis2-1.png" width="672" /></p>
</div>
</div>
<div id="section-interactive-app" class="section level2">
<h2><span class="header-section-number">7.2</span> Interactive App</h2>
<p>Go to the app: <a href="https://awstringer1.shinyapps.io/bayesian-tutorial/" class="uri">https://awstringer1.shinyapps.io/bayesian-tutorial/</a></p>
<p>The app lets you flip coins and estimate the probability of heads using Frequentist
and Bayesian methods. We haven’t covered estimation yet, but we have covered the
model for coin flipping in both contexts now, so you should be able to tell what’s
happening. Also shown are <em>interval estimates</em>, which measure the strength of
the conclusions about <span class="math inline">\(p\)</span> that are made based on the data and model. Narrower
interval estimates mean we’re more sure about the value of <span class="math inline">\(p\)</span>, after seeing the
data.</p>
<p>The app lets you change the following:</p>
<ul>
<li>The number of times you flip the coin,</li>
<li>The true probability of heads, <span class="math inline">\(p\)</span>,</li>
<li>Your prior belief about the probability of heads, the “prior mean”, and</li>
<li>The <em>strength</em> of your prior beliefs, as measured by the prior standard deviation.
Lower standard deviation means you’re <em>more sure</em> about the value of <span class="math inline">\(p\)</span>, before
seeing any flips.</li>
</ul>
<p>You should answer the following questions:</p>
<ol style="list-style-type: decimal">
<li><p>How many flips do you need before the Bayesian and frequentist inferences
agree closely? Does this depend on the true value of <span class="math inline">\(p\)</span>, your prior belief,
and the strength of your prior belief?</p></li>
<li><p>Intuitively: why are the Bayesian interval estimates narrower than the
frequentist ones? Is this always the case?</p></li>
<li><p>Can you “break” the Bayesian answer by expressing really strong and wrong
prior beliefs? Can you “fix” it by flipping the coin more times?</p></li>
</ol>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="section-supplement-to-evans-rosenthal-section-7-1.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="section-supplement-to-chapter-18.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["book.pdf", "book.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
