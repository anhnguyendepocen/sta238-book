<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 12 Predictive Modelling | STA238: Probability, Statistics, and Data Analysis</title>
  <meta name="description" content="This book represents part of the course materials for STA238 at the University of Toronto" />
  <meta name="generator" content="bookdown 0.13 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 12 Predictive Modelling | STA238: Probability, Statistics, and Data Analysis" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This book represents part of the course materials for STA238 at the University of Toronto" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 12 Predictive Modelling | STA238: Probability, Statistics, and Data Analysis" />
  
  <meta name="twitter:description" content="This book represents part of the course materials for STA238 at the University of Toronto" />
  

<meta name="author" content="Alison Gibbs and Alex Stringer" />


<meta name="date" content="2019-12-07" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="section-supplement-to-evans-rosenthal-section-7-2.html"/>
<link rel="next" href="section-installing-r-and-rstudio.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">STA238 University of Toronto</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="section-supplement-to-chapters-15-and-16.html"><a href="section-supplement-to-chapters-15-and-16.html"><i class="fa fa-check"></i><b>2</b> Supplement to Chapters 15 and 16</a><ul>
<li class="chapter" data-level="2.1" data-path="section-supplement-to-chapters-15-and-16.html"><a href="section-supplement-to-chapters-15-and-16.html#section-old-faithful"><i class="fa fa-check"></i><b>2.1</b> Old Faithful</a><ul>
<li class="chapter" data-level="2.1.1" data-path="section-supplement-to-chapters-15-and-16.html"><a href="section-supplement-to-chapters-15-and-16.html#section-read-in-the-data"><i class="fa fa-check"></i><b>2.1.1</b> Read in the data</a></li>
<li class="chapter" data-level="2.1.2" data-path="section-supplement-to-chapters-15-and-16.html"><a href="section-supplement-to-chapters-15-and-16.html#section-graphical-summaries"><i class="fa fa-check"></i><b>2.1.2</b> Graphical Summaries</a></li>
<li class="chapter" data-level="2.1.3" data-path="section-supplement-to-chapters-15-and-16.html"><a href="section-supplement-to-chapters-15-and-16.html#section-numerical-summaries-ch-16"><i class="fa fa-check"></i><b>2.1.3</b> Numerical Summaries (Ch 16)</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="section-supplement-to-chapters-15-and-16.html"><a href="section-supplement-to-chapters-15-and-16.html#section-drilling"><i class="fa fa-check"></i><b>2.2</b> Drilling</a><ul>
<li class="chapter" data-level="2.2.1" data-path="section-supplement-to-chapters-15-and-16.html"><a href="section-supplement-to-chapters-15-and-16.html#section-read-in-data"><i class="fa fa-check"></i><b>2.2.1</b> Read in data</a></li>
<li class="chapter" data-level="2.2.2" data-path="section-supplement-to-chapters-15-and-16.html"><a href="section-supplement-to-chapters-15-and-16.html#section-graphical-summaries-1"><i class="fa fa-check"></i><b>2.2.2</b> Graphical summaries</a></li>
<li class="chapter" data-level="2.2.3" data-path="section-supplement-to-chapters-15-and-16.html"><a href="section-supplement-to-chapters-15-and-16.html#section-numerical-summaries"><i class="fa fa-check"></i><b>2.2.3</b> Numerical summaries</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="section-supplement-to-chapters-15-and-16.html"><a href="section-supplement-to-chapters-15-and-16.html#section-exercises"><i class="fa fa-check"></i><b>2.3</b> Exercises</a></li>
<li class="chapter" data-level="2.4" data-path="section-supplement-to-chapters-15-and-16.html"><a href="section-supplement-to-chapters-15-and-16.html#section-extended-example-smoking-and-age-and-mortality"><i class="fa fa-check"></i><b>2.4</b> Extended example: smoking and age and mortality</a><ul>
<li class="chapter" data-level="2.4.1" data-path="section-supplement-to-chapters-15-and-16.html"><a href="section-supplement-to-chapters-15-and-16.html#section-exercises-1"><i class="fa fa-check"></i><b>2.4.1</b> Exercises</a></li>
<li class="chapter" data-level="2.4.2" data-path="section-supplement-to-chapters-15-and-16.html"><a href="section-supplement-to-chapters-15-and-16.html#section-exercises-2"><i class="fa fa-check"></i><b>2.4.2</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="section-supplement-to-chapters-15-and-16.html"><a href="section-supplement-to-chapters-15-and-16.html#section-case-study-rental-housing-in-toronto"><i class="fa fa-check"></i><b>2.5</b> Case study: rental housing in Toronto</a><ul>
<li class="chapter" data-level="2.5.1" data-path="section-supplement-to-chapters-15-and-16.html"><a href="section-supplement-to-chapters-15-and-16.html#section-load-the-data"><i class="fa fa-check"></i><b>2.5.1</b> Load the data</a></li>
<li class="chapter" data-level="2.5.2" data-path="section-supplement-to-chapters-15-and-16.html"><a href="section-supplement-to-chapters-15-and-16.html#section-analysis-i-what-does-the-data-look-like"><i class="fa fa-check"></i><b>2.5.2</b> Analysis I: what does the data look like?</a></li>
<li class="chapter" data-level="2.5.3" data-path="section-supplement-to-chapters-15-and-16.html"><a href="section-supplement-to-chapters-15-and-16.html#section-analysis-ii-do-different-wards-have-different-quality-housing"><i class="fa fa-check"></i><b>2.5.3</b> Analysis II: Do different wards have different quality housing?</a></li>
<li class="chapter" data-level="2.5.4" data-path="section-supplement-to-chapters-15-and-16.html"><a href="section-supplement-to-chapters-15-and-16.html#section-analysis-iii-trends-in-quality-over-time"><i class="fa fa-check"></i><b>2.5.4</b> Analysis III: trends in quality over time</a></li>
<li class="chapter" data-level="2.5.5" data-path="section-supplement-to-chapters-15-and-16.html"><a href="section-supplement-to-chapters-15-and-16.html#section-summary"><i class="fa fa-check"></i><b>2.5.5</b> Summary</a></li>
<li class="chapter" data-level="2.5.6" data-path="section-supplement-to-chapters-15-and-16.html"><a href="section-supplement-to-chapters-15-and-16.html#section-exercises-3"><i class="fa fa-check"></i><b>2.5.6</b> Exercises</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="section-supplement-to-chapters-13-and-14.html"><a href="section-supplement-to-chapters-13-and-14.html"><i class="fa fa-check"></i><b>3</b> Supplement to Chapters 13 and 14</a><ul>
<li class="chapter" data-level="3.1" data-path="section-supplement-to-chapters-13-and-14.html"><a href="section-supplement-to-chapters-13-and-14.html#section-law-of-large-numbers-chapter-13"><i class="fa fa-check"></i><b>3.1</b> Law of Large Numbers (Chapter 13)</a><ul>
<li class="chapter" data-level="3.1.1" data-path="section-supplement-to-chapters-13-and-14.html"><a href="section-supplement-to-chapters-13-and-14.html#section-extended-example-the-probability-of-heads"><i class="fa fa-check"></i><b>3.1.1</b> Extended example: the probability of heads</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="section-supplement-to-chapters-13-and-14.html"><a href="section-supplement-to-chapters-13-and-14.html#section-central-limit-theorem-chapter-14"><i class="fa fa-check"></i><b>3.2</b> Central Limit Theorem (Chapter 14)</a><ul>
<li class="chapter" data-level="3.2.1" data-path="section-supplement-to-chapters-13-and-14.html"><a href="section-supplement-to-chapters-13-and-14.html#section-extended-example-the-probability-of-heads-1"><i class="fa fa-check"></i><b>3.2.1</b> Extended example: the probability of heads</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="section-supplement-to-chapters-17-and-19.html"><a href="section-supplement-to-chapters-17-and-19.html"><i class="fa fa-check"></i><b>4</b> Supplement to Chapters 17 and 19</a><ul>
<li class="chapter" data-level="4.1" data-path="section-supplement-to-chapters-17-and-19.html"><a href="section-supplement-to-chapters-17-and-19.html#section-statistical-models-chapter-17"><i class="fa fa-check"></i><b>4.1</b> Statistical models (Chapter 17)</a><ul>
<li class="chapter" data-level="4.1.1" data-path="section-supplement-to-chapters-17-and-19.html"><a href="section-supplement-to-chapters-17-and-19.html#section-janka-hardness-data"><i class="fa fa-check"></i><b>4.1.1</b> Janka Hardness data</a></li>
<li class="chapter" data-level="4.1.2" data-path="section-supplement-to-chapters-17-and-19.html"><a href="section-supplement-to-chapters-17-and-19.html#section-extended-example-ttc-ridership-revenues"><i class="fa fa-check"></i><b>4.1.2</b> Extended example: TTC ridership revenues</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="section-supplement-to-chapters-17-and-19.html"><a href="section-supplement-to-chapters-17-and-19.html#section-unbiased-estimators-chapter-19"><i class="fa fa-check"></i><b>4.2</b> Unbiased Estimators (Chapter 19)</a><ul>
<li class="chapter" data-level="4.2.1" data-path="section-supplement-to-chapters-17-and-19.html"><a href="section-supplement-to-chapters-17-and-19.html#section-simulated-network-data"><i class="fa fa-check"></i><b>4.2.1</b> Simulated network data</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="section-supplement-to-chapter-20.html"><a href="section-supplement-to-chapter-20.html"><i class="fa fa-check"></i><b>5</b> Supplement to Chapter 20</a><ul>
<li class="chapter" data-level="5.1" data-path="section-supplement-to-chapter-20.html"><a href="section-supplement-to-chapter-20.html#section-efficiency-and-mean-square-error-chapter-20"><i class="fa fa-check"></i><b>5.1</b> Efficiency and Mean Square Error (Chapter 20)</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="section-supplement-to-evans-rosenthal-section-7-1.html"><a href="section-supplement-to-evans-rosenthal-section-7-1.html"><i class="fa fa-check"></i><b>6</b> Supplement to Evans &amp; Rosenthal Section 7.1</a><ul>
<li class="chapter" data-level="6.1" data-path="section-supplement-to-evans-rosenthal-section-7-1.html"><a href="section-supplement-to-evans-rosenthal-section-7-1.html#section-tutorial"><i class="fa fa-check"></i><b>6.1</b> Tutorial</a><ul>
<li class="chapter" data-level="6.1.1" data-path="section-supplement-to-evans-rosenthal-section-7-1.html"><a href="section-supplement-to-evans-rosenthal-section-7-1.html#section-frequentistlikelihood-perspective"><i class="fa fa-check"></i><b>6.1.1</b> Frequentist/Likelihood Perspective</a></li>
<li class="chapter" data-level="6.1.2" data-path="section-supplement-to-evans-rosenthal-section-7-1.html"><a href="section-supplement-to-evans-rosenthal-section-7-1.html#section-bayesian-inference-introduction"><i class="fa fa-check"></i><b>6.1.2</b> Bayesian Inference: introduction</a></li>
<li class="chapter" data-level="6.1.3" data-path="section-supplement-to-evans-rosenthal-section-7-1.html"><a href="section-supplement-to-evans-rosenthal-section-7-1.html#section-flipping-more-coins"><i class="fa fa-check"></i><b>6.1.3</b> Flipping More Coins</a></li>
<li class="chapter" data-level="6.1.4" data-path="section-supplement-to-evans-rosenthal-section-7-1.html"><a href="section-supplement-to-evans-rosenthal-section-7-1.html#section-visualization"><i class="fa fa-check"></i><b>6.1.4</b> Visualization</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="section-supplement-to-evans-rosenthal-section-7-1.html"><a href="section-supplement-to-evans-rosenthal-section-7-1.html#section-interactive-app"><i class="fa fa-check"></i><b>6.2</b> Interactive App</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="section-supplement-to-chapter-18.html"><a href="section-supplement-to-chapter-18.html"><i class="fa fa-check"></i><b>7</b> Supplement to Chapter 18</a><ul>
<li class="chapter" data-level="7.1" data-path="section-supplement-to-chapter-18.html"><a href="section-supplement-to-chapter-18.html#section-the-bootstrap-chapter-18"><i class="fa fa-check"></i><b>7.1</b> The Bootstrap (Chapter 18)</a><ul>
<li class="chapter" data-level="7.1.1" data-path="section-supplement-to-chapter-18.html"><a href="section-supplement-to-chapter-18.html#section-empirical-bootstrap-old-faithful-data"><i class="fa fa-check"></i><b>7.1.1</b> Empirical bootstrap: Old Faithful data</a></li>
<li class="chapter" data-level="7.1.2" data-path="section-supplement-to-chapter-18.html"><a href="section-supplement-to-chapter-18.html#section-parametric-bootstrap-software-data"><i class="fa fa-check"></i><b>7.1.2</b> Parametric Bootstrap: software data</a></li>
<li class="chapter" data-level="7.1.3" data-path="section-supplement-to-chapter-18.html"><a href="section-supplement-to-chapter-18.html#section-extended-example-the-standard-error-of-a-proportion"><i class="fa fa-check"></i><b>7.1.3</b> Extended example: the standard error of a proportion</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="section-supplement-to-chapter-21.html"><a href="section-supplement-to-chapter-21.html"><i class="fa fa-check"></i><b>8</b> Supplement to Chapter 21</a><ul>
<li class="chapter" data-level="8.1" data-path="section-supplement-to-chapter-21.html"><a href="section-supplement-to-chapter-21.html#section-maximum-likelihood-chapter-21"><i class="fa fa-check"></i><b>8.1</b> Maximum Likelihood (Chapter 21)</a><ul>
<li class="chapter" data-level="8.1.1" data-path="section-supplement-to-chapter-21.html"><a href="section-supplement-to-chapter-21.html#section-example-two-coins"><i class="fa fa-check"></i><b>8.1.1</b> Example: two coins</a></li>
<li class="chapter" data-level="8.1.2" data-path="section-supplement-to-chapter-21.html"><a href="section-supplement-to-chapter-21.html#section-example-unknown-coins-n-2"><i class="fa fa-check"></i><b>8.1.2</b> Example: unknown coins, <span class="math inline">\(n = 2\)</span></a></li>
<li class="chapter" data-level="8.1.3" data-path="section-supplement-to-chapter-21.html"><a href="section-supplement-to-chapter-21.html#section-example-unknown-coins-n-bigger-than-2"><i class="fa fa-check"></i><b>8.1.3</b> Example: unknown coins, <span class="math inline">\(n\)</span> bigger than <span class="math inline">\(2\)</span></a></li>
<li class="chapter" data-level="8.1.4" data-path="section-supplement-to-chapter-21.html"><a href="section-supplement-to-chapter-21.html#section-extended-example-rental-housing-in-toronto"><i class="fa fa-check"></i><b>8.1.4</b> Extended example: rental housing in Toronto</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="section-supplement-to-chapter-23-and-24.html"><a href="section-supplement-to-chapter-23-and-24.html"><i class="fa fa-check"></i><b>9</b> Supplement to Chapter 23 and 24</a><ul>
<li class="chapter" data-level="9.1" data-path="section-supplement-to-chapter-23-and-24.html"><a href="section-supplement-to-chapter-23-and-24.html#section-confidence-intervals-for-the-mean-chapter-23"><i class="fa fa-check"></i><b>9.1</b> Confidence Intervals for the Mean (Chapter 23)</a><ul>
<li class="chapter" data-level="9.1.1" data-path="section-supplement-to-chapter-23-and-24.html"><a href="section-supplement-to-chapter-23-and-24.html#section-simulation"><i class="fa fa-check"></i><b>9.1.1</b> Simulation</a></li>
<li class="chapter" data-level="9.1.2" data-path="section-supplement-to-chapter-23-and-24.html"><a href="section-supplement-to-chapter-23-and-24.html#section-gross-calorific-value-measurements-for-osterfeld-262de27"><i class="fa fa-check"></i><b>9.1.2</b> Gross calorific value measurements for Osterfeld 262DE27</a></li>
<li class="chapter" data-level="9.1.3" data-path="section-supplement-to-chapter-23-and-24.html"><a href="section-supplement-to-chapter-23-and-24.html#section-gross-calorific-value-measurements-for-daw-mill-258gb41"><i class="fa fa-check"></i><b>9.1.3</b> Gross calorific value measurements for Daw Mill 258GB41</a></li>
<li class="chapter" data-level="9.1.4" data-path="section-supplement-to-chapter-23-and-24.html"><a href="section-supplement-to-chapter-23-and-24.html#section-bootstrap-confidence-intervals"><i class="fa fa-check"></i><b>9.1.4</b> Bootstrap Confidence Intervals</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="section-supplement-to-chapter-23-and-24.html"><a href="section-supplement-to-chapter-23-and-24.html#section-more-on-confidence-intervals-chapter-24"><i class="fa fa-check"></i><b>9.2</b> More on confidence intervals (Chapter 24)</a><ul>
<li class="chapter" data-level="9.2.1" data-path="section-supplement-to-chapter-23-and-24.html"><a href="section-supplement-to-chapter-23-and-24.html#section-binomial-distribution"><i class="fa fa-check"></i><b>9.2.1</b> Binomial distribution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="section-extended-example-reasoning-about-goodness-of-fit.html"><a href="section-extended-example-reasoning-about-goodness-of-fit.html"><i class="fa fa-check"></i><b>10</b> Extended Example: Reasoning About Goodness of Fit</a><ul>
<li class="chapter" data-level="10.1" data-path="section-extended-example-reasoning-about-goodness-of-fit.html"><a href="section-extended-example-reasoning-about-goodness-of-fit.html#section-go-and-read-the-blog-post"><i class="fa fa-check"></i><b>10.1</b> Go and read the blog post</a></li>
<li class="chapter" data-level="10.2" data-path="section-extended-example-reasoning-about-goodness-of-fit.html"><a href="section-extended-example-reasoning-about-goodness-of-fit.html#section-distribution-of-last-digits"><i class="fa fa-check"></i><b>10.2</b> Distribution of last digits</a><ul>
<li class="chapter" data-level="10.2.1" data-path="section-extended-example-reasoning-about-goodness-of-fit.html"><a href="section-extended-example-reasoning-about-goodness-of-fit.html#section-read-in-the-data-1"><i class="fa fa-check"></i><b>10.2.1</b> Read in the data</a></li>
<li class="chapter" data-level="10.2.2" data-path="section-extended-example-reasoning-about-goodness-of-fit.html"><a href="section-extended-example-reasoning-about-goodness-of-fit.html#section-make-the-histogram"><i class="fa fa-check"></i><b>10.2.2</b> Make the histogram</a></li>
<li class="chapter" data-level="10.2.3" data-path="section-extended-example-reasoning-about-goodness-of-fit.html"><a href="section-extended-example-reasoning-about-goodness-of-fit.html#section-testing-goodness-of-fit-simulation"><i class="fa fa-check"></i><b>10.2.3</b> Testing goodness of fit: simulation</a></li>
<li class="chapter" data-level="10.2.4" data-path="section-extended-example-reasoning-about-goodness-of-fit.html"><a href="section-extended-example-reasoning-about-goodness-of-fit.html#section-testing-goodness-of-fit-math"><i class="fa fa-check"></i><b>10.2.4</b> Testing goodness of fit: math</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="section-supplement-to-evans-rosenthal-section-7-2.html"><a href="section-supplement-to-evans-rosenthal-section-7-2.html"><i class="fa fa-check"></i><b>11</b> Supplement to Evans &amp; Rosenthal Section 7.2</a><ul>
<li class="chapter" data-level="11.1" data-path="section-supplement-to-evans-rosenthal-section-7-2.html"><a href="section-supplement-to-evans-rosenthal-section-7-2.html#section-estimation-in-bayesian-inference-general-ideas"><i class="fa fa-check"></i><b>11.1</b> Estimation in Bayesian Inference: general ideas</a><ul>
<li class="chapter" data-level="11.1.1" data-path="section-supplement-to-evans-rosenthal-section-7-2.html"><a href="section-supplement-to-evans-rosenthal-section-7-2.html#section-the-prior"><i class="fa fa-check"></i><b>11.1.1</b> The Prior</a></li>
<li class="chapter" data-level="11.1.2" data-path="section-supplement-to-evans-rosenthal-section-7-2.html"><a href="section-supplement-to-evans-rosenthal-section-7-2.html#section-the-posterior"><i class="fa fa-check"></i><b>11.1.2</b> The Posterior</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="section-supplement-to-evans-rosenthal-section-7-2.html"><a href="section-supplement-to-evans-rosenthal-section-7-2.html#section-estimation-in-bayesian-inference-point-and-interval-estimation"><i class="fa fa-check"></i><b>11.2</b> Estimation in Bayesian Inference: point and interval estimation</a></li>
<li class="chapter" data-level="11.3" data-path="section-supplement-to-evans-rosenthal-section-7-2.html"><a href="section-supplement-to-evans-rosenthal-section-7-2.html#section-choosing-a-prior"><i class="fa fa-check"></i><b>11.3</b> Choosing a Prior</a><ul>
<li class="chapter" data-level="11.3.1" data-path="section-supplement-to-evans-rosenthal-section-7-2.html"><a href="section-supplement-to-evans-rosenthal-section-7-2.html#section-conjugate-priors"><i class="fa fa-check"></i><b>11.3.1</b> Conjugate Priors</a></li>
<li class="chapter" data-level="11.3.2" data-path="section-supplement-to-evans-rosenthal-section-7-2.html"><a href="section-supplement-to-evans-rosenthal-section-7-2.html#section-setting-hyperparameters-by-moment-matching"><i class="fa fa-check"></i><b>11.3.2</b> Setting Hyperparameters by moment-matching</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="section-predictive-modelling.html"><a href="section-predictive-modelling.html"><i class="fa fa-check"></i><b>12</b> Predictive Modelling</a><ul>
<li class="chapter" data-level="12.1" data-path="section-predictive-modelling.html"><a href="section-predictive-modelling.html#section-overview"><i class="fa fa-check"></i><b>12.1</b> Overview</a></li>
<li class="chapter" data-level="12.2" data-path="section-predictive-modelling.html"><a href="section-predictive-modelling.html#section-plug-in-prediction-coin-flipping"><i class="fa fa-check"></i><b>12.2</b> Plug-in prediction: coin flipping</a></li>
<li class="chapter" data-level="12.3" data-path="section-predictive-modelling.html"><a href="section-predictive-modelling.html#section-bayesian-prediction-coin-flipping"><i class="fa fa-check"></i><b>12.3</b> Bayesian Prediction: coin flipping</a></li>
<li class="chapter" data-level="12.4" data-path="section-predictive-modelling.html"><a href="section-predictive-modelling.html#section-extended-example-income-and-advertsing-datasets"><i class="fa fa-check"></i><b>12.4</b> Extended example: Income and Advertsing datasets</a></li>
<li class="chapter" data-level="12.5" data-path="section-predictive-modelling.html"><a href="section-predictive-modelling.html#section-extended-example-predicting-call-centre-wait-times"><i class="fa fa-check"></i><b>12.5</b> Extended example: predicting call centre wait times</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="section-installing-r-and-rstudio.html"><a href="section-installing-r-and-rstudio.html"><i class="fa fa-check"></i><b>13</b> Installing R and RStudio</a><ul>
<li class="chapter" data-level="13.1" data-path="section-installing-r-and-rstudio.html"><a href="section-installing-r-and-rstudio.html#section-installing-r"><i class="fa fa-check"></i><b>13.1</b> Installing R</a></li>
<li class="chapter" data-level="13.2" data-path="section-installing-r-and-rstudio.html"><a href="section-installing-r-and-rstudio.html#section-installing-rstudio"><i class="fa fa-check"></i><b>13.2</b> Installing RStudio</a></li>
<li class="chapter" data-level="13.3" data-path="section-installing-r-and-rstudio.html"><a href="section-installing-r-and-rstudio.html#section-using-rmarkdown"><i class="fa fa-check"></i><b>13.3</b> Using RMarkdown</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">STA238: Probability, Statistics, and Data Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="section-predictive-modelling" class="section level1">
<h1><span class="header-section-number">Chapter 12</span> Predictive Modelling</h1>
<p>This chapter is a supplementary tutorial on predictive modelling for STA238. The readings on this topic are <em>An Introduction to Statistical Learning with Applications in R</em> (ISLR), sections 2.1, 2.1.1, 2.2, 2.2.1, and 2.2.2; and <em>Probability and Statistics: the Science of Uncertainty</em>, section 7.2.4. The former has an overview of predictive modelling; the latter treats the Bayesian perspective.</p>
<p>The assigned exercises associated with this material are from ISLR Section 2.4. Do questions 1, 2, 3, 4, 5, 8, 9, 10.</p>
<div id="section-overview" class="section level2">
<h2><span class="header-section-number">12.1</span> Overview</h2>
<p>Up to this point in the course, we have been focussing on data analysis and inference. Analyzing the specific data that you have available helps answer the question “what happened?”. Combining the available data with computational and mathematical modelling helps answer the question “what can be learned about the world, based on what happened?”.</p>
<p>Predictive modelling focusses on answering “what is going to happen next?”. While the former two questions are of primary <em>scientific</em> interest, modern data science in business and industrial practice is often concerned with making predictions about what kind of data will be seen in the future. Predictive modelling, which is itself a science, is applied to address this question.</p>
<p>Often, the <em>point estimates</em> we get from our inferential procedures are themselves predictions. For example, we built a regression model of TTC ridership revenue over time back in chapter 4 of these supplementary notes. We <em>inferred</em> the slope of the regression line of revenue across years– but we could also use this line to make predictions for future years by simply extending the line to the year we want, and reading off its value.</p>
<p>However, when measuring the <em>uncertainty</em> in our predictions, the situation is different. When doing inference, we have one source of uncertainty that we choose to address: the variability in the data. If we sampled the dataset again, we would get different inferences, and so on. In prediction, we have two sources of uncertainty: that from the model that we use for prediction, which we inferred from the data and hence is subject to variability; and that from the act of sampling a new datapoint from this model.</p>
<p>ISLR referrs to these two sources of error as <em>reducible</em> and <em>irreducible</em>. We can always make our inferences less uncertain in theory by getting more data (reducibile uncertainty), but even if we knew the underlying data-generating mechanism perfectly, its output is still random (irreducible uncertainty)!</p>
<p>In fact: recall we measured the quality of an estimator <span class="math inline">\(\hat{\theta}\)</span> of a parameter <span class="math inline">\(\theta\)</span> using the Mean Squared Error (MSE): <span class="math inline">\(\mathbb{E}(\hat{\theta} - \theta)^{2}\)</span>. The analagous concept in prediction is the Mean Square Prediction Error (MSPE). Suppose we use some function <span class="math inline">\(\hat{Y} \equiv \hat{Y}(Y_{1},\ldots,Y_{n})\)</span> to predict a new <span class="math inline">\(Y_{n+1}\)</span> using a random sample <span class="math inline">\(Y_{1},\ldots,Y_{n}\)</span>, where <span class="math inline">\(Y_{i}, i = 1\ldots n+1\)</span> are drawn IID from the same (unknown) distribution. Then define:
<span class="math display">\[\begin{equation}
MSPE(\hat{Y}) = \mathbb{E}(\hat{Y}_{n+1} - Y_{n+1})^{2}
\end{equation}\]</span>
ISLR (page 19) makes an argument that in the simple model <span class="math inline">\(Y = \theta + \epsilon\)</span> where <span class="math inline">\(\epsilon\)</span> is a zero-mean “noise” random variable which is independent of both <span class="math inline">\(Y_{n+1}\)</span> and <span class="math inline">\(\hat{Y}_{n+1}\)</span> and <span class="math inline">\(\hat{\theta}\)</span> is an unbiased estimator of <span class="math inline">\(\theta\)</span>, that
<span class="math display">\[\begin{equation}\begin{aligned}
MSPE(\hat{Y}) &amp;= \mathbb{E}(\hat{\theta} - \theta)^{2} + \mathbb{E}(\epsilon^{2}) \\
&amp;= MSE(\hat{\theta}) + \text{Var}(\epsilon)
\end{aligned}\end{equation}\]</span></p>
<p><strong>Exercise</strong>: prove this formula. <em>Hint: look at what ISLR does, figure out the differences in notation between us and them, and fill in the details</em>.</p>
<p>The key takeaway is that <em>the prediction error comes from two sources: error in inferring the underlying data-generating mechanism, and error in drawing new data from this mechanism</em>.</p>
</div>
<div id="section-plug-in-prediction-coin-flipping" class="section level2">
<h2><span class="header-section-number">12.2</span> Plug-in prediction: coin flipping</h2>
To illustrate the idea of multiple sources of error, let’s briefly revist our now-classic coin flipping example. We are going to

<p>This is an example of a <strong>classification</strong> problem: we are predicting a binary event, something that either happens or it doesn’t. We’re either completely right, or completely wrong. Note that the MSPE here has a special form, because <span class="math inline">\(|\hat{Y} - Y| \in \left\{ 0,1 \right\}\)</span>.</p>
<p><strong>Exercise</strong>: show that the MSPE here is equal to the probability of making an incorrect prediction.</p>
<p>First let’s draw a sample. We’ll leave the parameters to be specified at the top of the program so you can play around with them:</p>
<div class="sourceCode" id="section-cb291"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb291-1" title="1"><span class="kw">set.seed</span>(<span class="dv">87886</span>)</a>
<a class="sourceLine" id="cb291-2" title="2">n &lt;-<span class="st"> </span><span class="dv">10</span> <span class="co"># Number of flips</span></a>
<a class="sourceLine" id="cb291-3" title="3">p0 &lt;-<span class="st"> </span><span class="fl">0.5</span> <span class="co"># True probability of heads</span></a>
<a class="sourceLine" id="cb291-4" title="4"></a>
<a class="sourceLine" id="cb291-5" title="5"><span class="co"># Draw the sample</span></a>
<a class="sourceLine" id="cb291-6" title="6">samp &lt;-<span class="st"> </span><span class="kw">rbinom</span>(n,<span class="dv">1</span>,p0)</a>
<a class="sourceLine" id="cb291-7" title="7">samp</a></code></pre></div>
<pre><code>##  [1] 1 0 0 0 1 0 1 0 0 1</code></pre>
<p>We have drawn <span class="math inline">\(n = 10\)</span> samples from a <span class="math inline">\(\text{Bernoulli}(p)\)</span> distribution with parameter <span class="math inline">\(p = 0.5\)</span>. If <span class="math inline">\(Y_{i}\)</span> is a <span class="math inline">\(0/1\)</span> indicator of the <span class="math inline">\(i^{th}\)</span> toss being heads, the likelihood function for <span class="math inline">\(p\)</span> is given by:
<span class="math display">\[\begin{equation}
L(p) = p^{\sum_{i=1}^{n}Y_{i}}(1-p)^{n - \sum_{i=1}^{n}Y_{i}}
\end{equation}\]</span></p>
<p><strong>Exercise</strong> (review): show that the maximum likelihood estimator is <span class="math inline">\(\hat{p} = \frac{1}{n}\sum_{i=1}^{n}Y_{i}\)</span>, the sample proportion of heads.</p>
<p>We can compute the MLE for our sample:</p>
<div class="sourceCode" id="section-cb293"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb293-1" title="1">pmle &lt;-<span class="st"> </span><span class="kw">mean</span>(samp)</a>
<a class="sourceLine" id="cb293-2" title="2">pmle</a></code></pre></div>
<pre><code>## [1] 0.4</code></pre>
<p>How to use this to make predictions? We need a rule—a function <span class="math inline">\(\hat{Y}\)</span>—that takes in our sample and predicts the next value. This construction is very similar in spirit to constructing an estimate of a parameter; we want something that makes intuitive sense and that has “desirable” statistical properties.</p>
<p>It can be shown that the prediction rule that minimizes the MSPE for this simple case of binary prediction is to simply predict the outcome (heads or tails, 0 or 1) that is the most probable under our estimated model. Our inferred model ascribes a <span class="math inline">\(40\%\)</span> chance of heads and <span class="math inline">\(60\%\)</span> chance of tails to each flip, so our prediction rule (for <em>this</em> sample) is simply <span class="math inline">\(\hat{Y} = 0\)</span>.</p>
<p><strong>Exercise</strong>: suppose that <span class="math inline">\(p_{0} = \hat{p} = 0.4\)</span>, that is, suppose we were able to perfectly infer the value of <span class="math inline">\(p\)</span> based on our sample and hence incur no model error. Show that the MSPE for our prediction rule is <span class="math inline">\(\mathbb{E}(\hat{Y} - Y)^{2} = p_{0} = 0.4\)</span>.</p>
<p><strong>Exercise</strong> (challenge): now suppose that the true value of <span class="math inline">\(p\)</span> is <span class="math inline">\(p_{0} = 0.5\)</span> (which it actually is, in our simulation). Show that the MSPE of our prediction rule equals <span class="math inline">\(0.5\)</span> when you incorporate the model error into the calculation. <em>Hint</em>: recall the “tower property” of expectation: for any two random variables <span class="math inline">\(X,Y\)</span>,
<span class="math display">\[\begin{equation}
\mathbb{E}(X) = \mathbb{E}_{Y}\mathbb{E}_{X|Y}(X)
\end{equation}\]</span>
That is, you can first take the expectation with respect to one random variable <em>conditional</em> on another, and then take the expectation of this expression with respect to the second random variable. Use this to separate the model variability from the sampling variability.</p>
Let’s assess the MSPE using a prediction. We’re going to repeat the following procedure a bunch of times:

<div class="sourceCode" id="section-cb295"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb295-1" title="1"><span class="kw">set.seed</span>(<span class="dv">874327086</span>)</a>
<a class="sourceLine" id="cb295-2" title="2">n &lt;-<span class="st"> </span><span class="dv">10</span> <span class="co"># Number of flips</span></a>
<a class="sourceLine" id="cb295-3" title="3">p0 &lt;-<span class="st"> </span><span class="fl">0.5</span> <span class="co"># True probability of heads</span></a>
<a class="sourceLine" id="cb295-4" title="4">B &lt;-<span class="st"> </span><span class="dv">1000</span> <span class="co"># Number of simulations to do</span></a>
<a class="sourceLine" id="cb295-5" title="5"></a>
<a class="sourceLine" id="cb295-6" title="6">do_simulation &lt;-<span class="st"> </span><span class="cf">function</span>(n,p) {</a>
<a class="sourceLine" id="cb295-7" title="7">  pmle &lt;-<span class="st"> </span><span class="kw">mean</span>(<span class="kw">rbinom</span>(n,<span class="dv">1</span>,p)) <span class="co"># Compute the MLE</span></a>
<a class="sourceLine" id="cb295-8" title="8">  newy &lt;-<span class="st"> </span><span class="kw">rbinom</span>(<span class="dv">1</span>,<span class="dv">1</span>,p) <span class="co"># Draw a new value from the true distribution</span></a>
<a class="sourceLine" id="cb295-9" title="9">  <span class="cf">if</span> (pmle <span class="op">&gt;=</span><span class="st"> </span><span class="fl">0.5</span>) {</a>
<a class="sourceLine" id="cb295-10" title="10">    predy &lt;-<span class="st"> </span><span class="dv">1</span> <span class="co"># Predict 1 if pmle &gt;= 0.5</span></a>
<a class="sourceLine" id="cb295-11" title="11">  } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb295-12" title="12">    predy &lt;-<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb295-13" title="13">  }</a>
<a class="sourceLine" id="cb295-14" title="14">  (predy <span class="op">-</span><span class="st"> </span>newy)<span class="op">^</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb295-15" title="15">}</a>
<a class="sourceLine" id="cb295-16" title="16"></a>
<a class="sourceLine" id="cb295-17" title="17"><span class="co"># Do the simulation </span></a>
<a class="sourceLine" id="cb295-18" title="18">sim_mspe &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span>B <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb295-19" title="19"><span class="st">  </span><span class="kw">map</span>(<span class="op">~</span><span class="kw">do_simulation</span>(<span class="dt">n =</span> n,<span class="dt">p =</span> p0)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb295-20" title="20"><span class="st">  </span><span class="kw">reduce</span>(c) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb295-21" title="21"><span class="st">  </span><span class="kw">mean</span>()</a>
<a class="sourceLine" id="cb295-22" title="22"></a>
<a class="sourceLine" id="cb295-23" title="23">sim_mspe</a></code></pre></div>
<pre><code>## [1] 0.498</code></pre>
<p>Notice that it is close to <span class="math inline">\(0.5\)</span>, not <span class="math inline">\(0.4\)</span>– the model error matters, as you saw in the above theoretical exercise. Actually, to drive this point home…</p>
<p><strong>Exercise</strong>: what did I do differently in the below code, compared to the above?</p>
<div class="sourceCode" id="section-cb297"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb297-1" title="1"><span class="kw">set.seed</span>(<span class="dv">874327086</span>)</a>
<a class="sourceLine" id="cb297-2" title="2">n &lt;-<span class="st"> </span><span class="dv">10</span> <span class="co"># Number of flips</span></a>
<a class="sourceLine" id="cb297-3" title="3">p0 &lt;-<span class="st"> </span><span class="fl">0.5</span> <span class="co"># True probability of heads</span></a>
<a class="sourceLine" id="cb297-4" title="4">B &lt;-<span class="st"> </span><span class="dv">1000</span> <span class="co"># Number of simulations to do</span></a>
<a class="sourceLine" id="cb297-5" title="5"></a>
<a class="sourceLine" id="cb297-6" title="6">do_simulation &lt;-<span class="st"> </span><span class="cf">function</span>(n,p) {</a>
<a class="sourceLine" id="cb297-7" title="7">  pmle &lt;-<span class="st"> </span><span class="kw">mean</span>(<span class="kw">rbinom</span>(n,<span class="dv">1</span>,p)) <span class="co"># Compute the MLE</span></a>
<a class="sourceLine" id="cb297-8" title="8">  newy &lt;-<span class="st"> </span><span class="kw">rbinom</span>(<span class="dv">1</span>,<span class="dv">1</span>,pmle) <span class="co"># Draw a new value from the true distribution</span></a>
<a class="sourceLine" id="cb297-9" title="9">  <span class="cf">if</span> (pmle <span class="op">&gt;=</span><span class="st"> </span><span class="fl">0.5</span>) {</a>
<a class="sourceLine" id="cb297-10" title="10">    predy &lt;-<span class="st"> </span><span class="dv">1</span> <span class="co"># Predict 1 if pmle &gt;= 0.5</span></a>
<a class="sourceLine" id="cb297-11" title="11">  } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb297-12" title="12">    predy &lt;-<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb297-13" title="13">  }</a>
<a class="sourceLine" id="cb297-14" title="14">  (predy <span class="op">-</span><span class="st"> </span>newy)<span class="op">^</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb297-15" title="15">}</a>
<a class="sourceLine" id="cb297-16" title="16"></a>
<a class="sourceLine" id="cb297-17" title="17"><span class="co"># Do the simulation </span></a>
<a class="sourceLine" id="cb297-18" title="18">sim_mspe2 &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span>B <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb297-19" title="19"><span class="st">  </span><span class="kw">map</span>(<span class="op">~</span><span class="kw">do_simulation</span>(<span class="dt">n =</span> n,<span class="dt">p =</span> p0)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb297-20" title="20"><span class="st">  </span><span class="kw">reduce</span>(c) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb297-21" title="21"><span class="st">  </span><span class="kw">mean</span>()</a>
<a class="sourceLine" id="cb297-22" title="22"></a>
<a class="sourceLine" id="cb297-23" title="23">sim_mspe2</a></code></pre></div>
<pre><code>## [1] 0.389</code></pre>
<p>Figure out why it’s lower, <em>computationally</em> (what did I do differently in the code), <em>mathematically</em> (relate it to the above exercises), and <em>intuitively</em> (explain in words what the difference is).</p>
</div>
<div id="section-bayesian-prediction-coin-flipping" class="section level2">
<h2><span class="header-section-number">12.3</span> Bayesian Prediction: coin flipping</h2>
<p>We saw above that making predictions using our frequentist-inferred models for the observed data wasn’t that complicated (just “plug in” the inferred model parameters), but assessing the <em>uncertainty</em> in the predictions was challenging because the uncertainty comes from multiple different sources, and it is not always clear how to effectively combine these.</p>
<p>One more systematic way to quantify the uncertainty in a prediction, conditional on the observed sample is, well, to <em>literally</em> do this, via the conditional distribution <span class="math inline">\(\pi(Y_{n+1}|Y_{1},\ldots,Y_{n})\)</span>. The mean of this distribution says “what will happen on average, given what we have observed?”, and the standard deviation of this distribution quantifies how close future values are to be concentrated around this mean, which seems like a perfectly reasonable measure of the uncertainty in a prediction.</p>
<p>So what’s the problem? If we’re in the frequentist paradigm, there is a big one: the data are IID! So <span class="math inline">\(Y_{n+1}\)</span> is <em>statistically independent</em> of <span class="math inline">\(Y_{1},\ldots,Y_{n}\)</span>, and
<span class="math display">\[\begin{equation}
\pi(Y_{n+1}|Y_{1},\ldots,Y_{n}) = \pi(Y_{n+1})
\end{equation}\]</span>
Under this paradigm, the sample <em>provides no information</em> about the distribution of a new datapoint.</p>
<p>In the Bayesian paradigm, things are a bit different. Following Evans and Rosenthal Section 7.2.4, we consider the <strong>posterior predictive distribution</strong>:
<span class="math display">\[\begin{equation}
\pi(Y_{n+1}|Y_{1},\ldots,Y_{n}) = \int_{0}^{1}\pi(Y_{n+1}|p)\pi(p|Y_{1},\ldots,Y_{n})dp
\end{equation}\]</span>
where <span class="math inline">\(\pi(p|Y_{1},\ldots,Y_{n})\)</span> is the posterior distribution of <span class="math inline">\(p\)</span> given the sample, and <span class="math inline">\(\pi(Y_{n+1}|p)\)</span> is simply the likelihood for each value of <span class="math inline">\(p\)</span>, and for a single datapoint.</p>
This has the following attractive properties:

<p>Let’s see what this looks like for the coin flipping example. Recall the setup: we use a <span class="math inline">\(\text{Beta}(\alpha,\beta)\)</span> prior for the parameter <span class="math inline">\(p\)</span>, which leads to the posterior:
<span class="math display">\[\begin{equation}
\pi(p|Y_{1},\ldots,Y_{n}) = \text{Beta}\left(n\bar{Y} + \alpha,n(1 - \bar{Y}) + \beta\right)
\end{equation}\]</span>
Evans and Rosenthal derive the posterior predictive distribution using messier, but similar algebra to how we derived the posterior:
<span class="math display">\[\begin{equation}
Y_{n+1} | Y_{1},\ldots,Y_{n} \sim \text{Bernoulli}\left(\frac{n\bar{Y} + \alpha}{n + \alpha + \beta}\right)
\end{equation}\]</span>
<strong>Exercise</strong>: derive the formula for the posterior predictive as reported by Evans and Rosenthal.</p>
<p>We use the posterior predictive mode for prediction. Since the posterior predictive distribution is only defined at two values, <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>, the mode is simply whichever value has the higher posterior predictive density.</p>
<p><strong>Exercise</strong>: prove Evans and Rosenthal’s formula for the posterior predictive mode:
<span class="math display">\[\begin{equation}
\hat{Y}_{n+1} | Y_{1},\ldots,Y_{n} = \begin{cases} 1 \ \text{if} \ \frac{n\bar{Y} + \alpha}{n + \alpha + \beta} \geq \frac{n(1-\bar{Y}) + \beta}{n + \alpha + \beta} \\ 0 \ \text{else} \end{cases}
\end{equation}\]</span>
<em>Hint: start by writing out the Bernoulli density</em>.</p>
<p>How does the use of the posterior predictive mode do in terms of MSPE?
Consider the setup from chapter 11 of these supplementary notes, where we put a <span class="math inline">\(\text{Beta}(12,12)\)</span> prior on <span class="math inline">\(p\)</span>. Let’s simulate the MSPE again, but using this for prediction:</p>
<div class="sourceCode" id="section-cb299"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb299-1" title="1"><span class="kw">set.seed</span>(<span class="dv">874327086</span>)</a>
<a class="sourceLine" id="cb299-2" title="2">n &lt;-<span class="st"> </span><span class="dv">10</span> <span class="co"># Number of flips</span></a>
<a class="sourceLine" id="cb299-3" title="3">p0 &lt;-<span class="st"> </span><span class="fl">0.5</span> <span class="co"># True probability of heads</span></a>
<a class="sourceLine" id="cb299-4" title="4">alpha &lt;-<span class="st"> </span><span class="dv">12</span> <span class="co"># Prior</span></a>
<a class="sourceLine" id="cb299-5" title="5">beta &lt;-<span class="st"> </span><span class="dv">12</span> <span class="co"># Prior</span></a>
<a class="sourceLine" id="cb299-6" title="6">B &lt;-<span class="st"> </span><span class="dv">1000</span> <span class="co"># Number of simulations to do</span></a>
<a class="sourceLine" id="cb299-7" title="7"></a>
<a class="sourceLine" id="cb299-8" title="8">do_simulation_bayes &lt;-<span class="st"> </span><span class="cf">function</span>(n,p) {</a>
<a class="sourceLine" id="cb299-9" title="9">  pmle &lt;-<span class="st"> </span><span class="kw">mean</span>(<span class="kw">rbinom</span>(n,<span class="dv">1</span>,p)) <span class="co"># Compute the MLE</span></a>
<a class="sourceLine" id="cb299-10" title="10">  newy &lt;-<span class="st"> </span><span class="kw">rbinom</span>(<span class="dv">1</span>,<span class="dv">1</span>,p) <span class="co"># Draw a new value from the true distribution</span></a>
<a class="sourceLine" id="cb299-11" title="11">  postparam &lt;-<span class="st"> </span>(n <span class="op">*</span><span class="st"> </span>pmle <span class="op">+</span><span class="st"> </span>alpha) <span class="op">/</span><span class="st"> </span>(n <span class="op">+</span><span class="st"> </span>alpha <span class="op">+</span><span class="st"> </span>beta)</a>
<a class="sourceLine" id="cb299-12" title="12">  <span class="cf">if</span> (postparam <span class="op">&gt;=</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>postparam)) {</a>
<a class="sourceLine" id="cb299-13" title="13">    predy &lt;-<span class="st"> </span><span class="dv">1</span> <span class="co"># Predict 1 if postparam &gt;= (1 - postparam) (i.e. postparam &gt;= 0.5)</span></a>
<a class="sourceLine" id="cb299-14" title="14">  } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb299-15" title="15">    predy &lt;-<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb299-16" title="16">  }</a>
<a class="sourceLine" id="cb299-17" title="17">  (predy <span class="op">-</span><span class="st"> </span>newy)<span class="op">^</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb299-18" title="18">}</a>
<a class="sourceLine" id="cb299-19" title="19"></a>
<a class="sourceLine" id="cb299-20" title="20"><span class="co"># Do the simulation </span></a>
<a class="sourceLine" id="cb299-21" title="21">sim_mspe_bayes &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span>B <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb299-22" title="22"><span class="st">  </span><span class="kw">map</span>(<span class="op">~</span><span class="kw">do_simulation_bayes</span>(<span class="dt">n =</span> n,<span class="dt">p =</span> p0)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb299-23" title="23"><span class="st">  </span><span class="kw">reduce</span>(c) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb299-24" title="24"><span class="st">  </span><span class="kw">mean</span>()</a>
<a class="sourceLine" id="cb299-25" title="25"></a>
<a class="sourceLine" id="cb299-26" title="26">sim_mspe_bayes</a></code></pre></div>
<pre><code>## [1] 0.498</code></pre>
<p>Notice anything odd about these results compared to what we got using the frequentist approach?</p>
<p>It turns out that if the prior is symmetric (<span class="math inline">\(\alpha = \beta\)</span>), then the posterior mode <em>exactly equals</em> the prediction rule we obtained by minimizing the MSPE. If the prior is not symmetric, they won’t be the same– so which do you expect to be better?</p>
</div>
<div id="section-extended-example-income-and-advertsing-datasets" class="section level2">
<h2><span class="header-section-number">12.4</span> Extended example: Income and Advertsing datasets</h2>
<p>Let’s recreate the examples from chapter 2 of ISLR.</p>
</div>
<div id="section-extended-example-predicting-call-centre-wait-times" class="section level2">
<h2><span class="header-section-number">12.5</span> Extended example: predicting call centre wait times</h2>
<p>The coin-flipping example is illustrative, but too simple to be interesting.
For a given dataset, we either predict heads, or tails, the same for all future
coins!</p>
<p>For a more interesting example, we’ll fit a curve to Toronto 311 contact centre wait times.
We’ll use a minor extension of the linear regression we saw in chapter 4 of these
supplementary notes. It’s more important to focus on what we’re doing (prediction)
than how we’re doing it (linear regression with polynomials). Materials in this tutorial
are taken from the (much) more advanced tutorial by Alex on this stuff, <a href="https://awstringer1.github.io/leaf2018/intro-to-bayesian.html#full-example-bayesian-regression">here</a>.</p>
<p>311 is a <a href="https://www.toronto.ca/home/311-toronto-at-your-service/">service</a> operated by the City of Toronto in which residents can call (dial *311) and submit service requests for things like potholes, excess garbage pickup, downed trees, and so on. People call in to the service and speak to an actual human being in a contact centre. When you call, sometimes you have to wait.</p>
<p>Data on the daily average wait time for such calls for the period from December 28th, 2009 to January 2nd, 2011 is available from Open Data Toronto <a href="https://www.toronto.ca/city-government/data-research-maps/open-data/open-data-catalogue/#9d74296c-05f1-13b0-8129-efaffd623963">here</a>, and stored on <a href="https://media.githubusercontent.com/media/awstringer1/leaf2018/gh-pages/datasets/contact-centre.csv">github</a> by us for your use.</p>
<p>Let’s read in and visualize the data:</p>
<div class="sourceCode" id="section-cb301"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb301-1" title="1"><span class="co"># Read data</span></a>
<a class="sourceLine" id="cb301-2" title="2">contactdat &lt;-<span class="st"> </span>readr<span class="op">::</span><span class="kw">read_csv</span>(</a>
<a class="sourceLine" id="cb301-3" title="3">  <span class="dt">file =</span> <span class="st">&quot;https://media.githubusercontent.com/media/awstringer1/leaf2018/gh-pages/datasets/contact-centre.csv&quot;</span>,</a>
<a class="sourceLine" id="cb301-4" title="4">  <span class="dt">col_names =</span> <span class="ot">TRUE</span>,</a>
<a class="sourceLine" id="cb301-5" title="5">  <span class="dt">col_types =</span> <span class="st">&quot;cd&quot;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb301-6" title="6"><span class="st">  </span><span class="kw">mutate_at</span>(<span class="st">&quot;date&quot;</span>,lubridate<span class="op">::</span>mdy)</a>
<a class="sourceLine" id="cb301-7" title="7"></a>
<a class="sourceLine" id="cb301-8" title="8"><span class="kw">glimpse</span>(contactdat)</a></code></pre></div>
<pre><code>## Observations: 371
## Variables: 2
## $ date         &lt;date&gt; 2009-12-28, 2009-12-29, 2009-12-30, 2009-12-31, 20…
## $ answer_speed &lt;dbl&gt; 55, 67, 56, 15, 9, 62, 51, 20, 16, 14, 23, 33, 14, …</code></pre>
<div class="sourceCode" id="section-cb303"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb303-1" title="1">contactdat <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb303-2" title="2"><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> date,<span class="dt">y =</span> answer_speed)) <span class="op">+</span></a>
<a class="sourceLine" id="cb303-3" title="3"><span class="st">  </span><span class="kw">theme_classic</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb303-4" title="4"><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">pch =</span> <span class="dv">21</span>,<span class="dt">colour =</span> <span class="st">&quot;black&quot;</span>,<span class="dt">fill =</span> <span class="st">&quot;orange&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb303-5" title="5"><span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Average Wait Time by Day, Toronto 311 Contact Centre&quot;</span>,</a>
<a class="sourceLine" id="cb303-6" title="6">       <span class="dt">x =</span> <span class="st">&quot;Date&quot;</span>,</a>
<a class="sourceLine" id="cb303-7" title="7">       <span class="dt">y =</span> <span class="st">&quot;Wait Time&quot;</span>)</a></code></pre></div>
<p><img src="book_files/figure-html/contact-1-1.png" width="672" /></p>
<p>Woah! That spike in wait times in July is terrible. If we were analyzing these data
with the intent of informing any actual policy, we would contact the 311 office
and ask what the heck happened there before proceeding.</p>
<p>Let’s proceed. To mitigate the drastic spike, consider a log transformation of
the <code>answer_speed</code>:</p>
<div class="sourceCode" id="section-cb304"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb304-1" title="1">contactdat <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb304-2" title="2"><span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> date,<span class="dt">y =</span> <span class="kw">log</span>(answer_speed))) <span class="op">+</span></a>
<a class="sourceLine" id="cb304-3" title="3"><span class="st">  </span><span class="kw">theme_classic</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb304-4" title="4"><span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">pch =</span> <span class="dv">21</span>,<span class="dt">colour =</span> <span class="st">&quot;black&quot;</span>,<span class="dt">fill =</span> <span class="st">&quot;orange&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb304-5" title="5"><span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Average Wait Time by Day, Toronto 311 Contact Centre&quot;</span>,</a>
<a class="sourceLine" id="cb304-6" title="6">       <span class="dt">x =</span> <span class="st">&quot;Date&quot;</span>,</a>
<a class="sourceLine" id="cb304-7" title="7">       <span class="dt">y =</span> <span class="st">&quot;log(Wait Time)&quot;</span>)</a></code></pre></div>
<p><img src="book_files/figure-html/contact-2-1.png" width="672" /></p>
<p>There’s a lot of variance, but it looks like there might be a kind of quadratic trend.</p>
<p>Recall the simple linear model from Chapter 4 of these supplementary notes:
<span class="math display">\[
y_{i} = \beta_{0} + \beta_{1}x_{i} + \epsilon_{i}
\]</span>
where <span class="math inline">\(y_{i}\)</span> is the average wait time on the <span class="math inline">\(i^{th}\)</span> date, <span class="math inline">\(x_{i}\)</span> is a suitable numerical value for the <span class="math inline">\(i^{th}\)</span> date (e.g. number of days since some reference point), and <span class="math inline">\(\epsilon_{i} \sim N(0,\sigma^{2})\)</span> is the error of the <span class="math inline">\(i^{th}\)</span> observation about its mean. Because it looks
like the trend in the data might be quadratic, not linear, we can extend this slightly:
<span class="math display">\[
y_{i} = \beta_{0} + \beta_{1}x_{i} + \beta_{2}x_{i}^{2} + \epsilon_{i}
\]</span>
In general, it’s not too much of an extension to consider models of the form
<span class="math display">\[
y_{i} = \beta_{0} + \sum_{k=1}^{p}\beta_{k}x_{i}^{k} + \epsilon_{i}
\]</span>
The higher we choose <span class="math inline">\(p\)</span>, the degree of polynomial, the closer we will fit to
the observed data, and the better we will be able to predict the datapoints we have
already seen. However, paradoxically, models that are <em>too complicated</em> will actually
predict <em>new</em> data worse! This is due in part to the <em>variance-bias tradeoff</em> discussed in
this week’s readings in ISLR.</p>
<p>Let’s fit the model(s) and assess the in-sample mean-square prediction error.</p>
<div class="sourceCode" id="section-cb305"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb305-1" title="1"><span class="co"># THIS CODE IS ADVANCED-- the idea is important, but you won&#39;t be</span></a>
<a class="sourceLine" id="cb305-2" title="2"><span class="co"># tested on the following CODE.</span></a>
<a class="sourceLine" id="cb305-3" title="3">contact_std &lt;-<span class="st"> </span>contactdat <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb305-4" title="4"><span class="st">  </span><span class="kw">mutate_at</span>(<span class="st">&quot;date&quot;</span>,as.numeric) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb305-5" title="5"><span class="st">  </span><span class="kw">mutate_at</span>(<span class="st">&quot;answer_speed&quot;</span>,log) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb305-6" title="6"><span class="st">  </span><span class="kw">mutate_all</span>(<span class="kw">list</span>(<span class="op">~</span>(. <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(.)) <span class="op">/</span><span class="st"> </span><span class="kw">sd</span>(.)))</a>
<a class="sourceLine" id="cb305-7" title="7"></a>
<a class="sourceLine" id="cb305-8" title="8"><span class="co"># Fit a polynomial regression of degree p</span></a>
<a class="sourceLine" id="cb305-9" title="9">fit_poly_regression &lt;-<span class="st"> </span><span class="cf">function</span>(p) {</a>
<a class="sourceLine" id="cb305-10" title="10">  <span class="kw">lm</span>(answer_speed <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(date,<span class="dt">degree =</span> p,<span class="dt">raw =</span> <span class="ot">TRUE</span>),</a>
<a class="sourceLine" id="cb305-11" title="11">     <span class="dt">data =</span> contact_std)</a>
<a class="sourceLine" id="cb305-12" title="12">}</a>
<a class="sourceLine" id="cb305-13" title="13"></a>
<a class="sourceLine" id="cb305-14" title="14">contactmod_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">fit_poly_regression</span>(<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb305-15" title="15">contactmod_<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">fit_poly_regression</span>(<span class="dv">2</span>)</a>
<a class="sourceLine" id="cb305-16" title="16">contactmod_<span class="dv">10</span> &lt;-<span class="st"> </span><span class="kw">fit_poly_regression</span>(<span class="dv">10</span>)</a>
<a class="sourceLine" id="cb305-17" title="17">contactmod_<span class="dv">100</span> &lt;-<span class="st"> </span><span class="kw">fit_poly_regression</span>(<span class="dv">100</span>)</a>
<a class="sourceLine" id="cb305-18" title="18"></a>
<a class="sourceLine" id="cb305-19" title="19"><span class="co"># Plot of the predictions</span></a>
<a class="sourceLine" id="cb305-20" title="20">prediction_plot &lt;-<span class="st"> </span><span class="cf">function</span>(mod) {</a>
<a class="sourceLine" id="cb305-21" title="21">  contact_std <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb305-22" title="22"><span class="st">    </span><span class="kw">mutate</span>(<span class="dt">pred =</span> <span class="kw">predict</span>(mod)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb305-23" title="23"><span class="st">    </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> date)) <span class="op">+</span></a>
<a class="sourceLine" id="cb305-24" title="24"><span class="st">    </span><span class="kw">theme_classic</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb305-25" title="25"><span class="st">    </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">y =</span> answer_speed),<span class="dt">pch =</span> <span class="dv">21</span>,<span class="dt">colour =</span> <span class="st">&quot;black&quot;</span>,<span class="dt">fill =</span> <span class="st">&quot;orange&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb305-26" title="26"><span class="st">    </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> pred),<span class="dt">colour =</span> <span class="st">&quot;purple&quot;</span>)</a>
<a class="sourceLine" id="cb305-27" title="27">}</a>
<a class="sourceLine" id="cb305-28" title="28"></a>
<a class="sourceLine" id="cb305-29" title="29">cowplot<span class="op">::</span><span class="kw">plot_grid</span>(</a>
<a class="sourceLine" id="cb305-30" title="30">  <span class="kw">prediction_plot</span>(contactmod_<span class="dv">1</span>),</a>
<a class="sourceLine" id="cb305-31" title="31">  <span class="kw">prediction_plot</span>(contactmod_<span class="dv">2</span>),</a>
<a class="sourceLine" id="cb305-32" title="32">  <span class="kw">prediction_plot</span>(contactmod_<span class="dv">10</span>),</a>
<a class="sourceLine" id="cb305-33" title="33">  <span class="kw">prediction_plot</span>(contactmod_<span class="dv">100</span>),</a>
<a class="sourceLine" id="cb305-34" title="34">  <span class="dt">nrow =</span> <span class="dv">2</span></a>
<a class="sourceLine" id="cb305-35" title="35">)</a></code></pre></div>
<p><img src="book_files/figure-html/fit-models-1-1.png" width="672" /></p>
<p><strong>Exercise</strong>: I deliberately didn’t label the plots. Can you guess which plot is
from which degree of polynomial? Remember that <em>higher</em> degree polynomials should
be more wiggly than lower degrees, and should give better <em>in-sample</em> predictions
than lower degrees.</p>
<p>Indeed, if we compute the MSPE for each of these models, we find the higher the degree,
the better the fit <em>to the data we have observed</em>:</p>
<div class="sourceCode" id="section-cb306"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb306-1" title="1">compute_mspe_polymod &lt;-<span class="st"> </span><span class="cf">function</span>(mod) {</a>
<a class="sourceLine" id="cb306-2" title="2">  <span class="kw">mean</span>(( <span class="kw">predict</span>(mod) <span class="op">-</span><span class="st"> </span>contact_std<span class="op">$</span>answer_speed )<span class="op">^</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb306-3" title="3">}</a>
<a class="sourceLine" id="cb306-4" title="4"></a>
<a class="sourceLine" id="cb306-5" title="5"><span class="kw">compute_mspe_polymod</span>(contactmod_<span class="dv">1</span>)</a></code></pre></div>
<pre><code>## [1] 0.9847131</code></pre>
<div class="sourceCode" id="section-cb308"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb308-1" title="1"><span class="kw">compute_mspe_polymod</span>(contactmod_<span class="dv">2</span>)</a></code></pre></div>
<pre><code>## [1] 0.7439483</code></pre>
<div class="sourceCode" id="section-cb310"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb310-1" title="1"><span class="kw">compute_mspe_polymod</span>(contactmod_<span class="dv">10</span>)</a></code></pre></div>
<pre><code>## [1] 0.5907413</code></pre>
<div class="sourceCode" id="section-cb312"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb312-1" title="1"><span class="kw">compute_mspe_polymod</span>(contactmod_<span class="dv">100</span>)</a></code></pre></div>
<pre><code>## [1] 0.466288</code></pre>
The problem with this is that the MSPE is supposed to be for predicting a <em>new</em>
datapoint, not one that you’ve already seen! To get a better estimate of the real
MSPE, and hence tell how the algorithm might do on new datapoints, we can bootstrap
it. We’ll use a slightly different kind of bootstrap, called <strong>cross-validation</strong>,
where we

<p>There are many types of cross-validation; this one is called “K-fold” cross validation.</p>
<p>We can implement this as follows:</p>
<div class="sourceCode" id="section-cb314"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb314-1" title="1"><span class="kw">set.seed</span>(<span class="dv">809796857</span>)</a>
<a class="sourceLine" id="cb314-2" title="2">do_crossval &lt;-<span class="st"> </span><span class="cf">function</span>(p,<span class="dt">K =</span> <span class="dv">10</span>) {</a>
<a class="sourceLine" id="cb314-3" title="3">  n &lt;-<span class="st"> </span><span class="kw">nrow</span>(contact_std)</a>
<a class="sourceLine" id="cb314-4" title="4">  mspevec &lt;-<span class="st"> </span><span class="kw">numeric</span>(K)</a>
<a class="sourceLine" id="cb314-5" title="5">  <span class="co"># Split the data into K &quot;folds&quot;</span></a>
<a class="sourceLine" id="cb314-6" title="6">  <span class="co"># First randomly shuffle the rows:</span></a>
<a class="sourceLine" id="cb314-7" title="7">  contact_std_shuffle &lt;-<span class="st"> </span>contact_std[<span class="kw">sample.int</span>(n), ]</a>
<a class="sourceLine" id="cb314-8" title="8">  <span class="co"># Then split the data in order:</span></a>
<a class="sourceLine" id="cb314-9" title="9">  datachunks &lt;-<span class="st"> </span><span class="kw">split</span>(contact_std_shuffle,<span class="kw">rep</span>(<span class="dv">1</span><span class="op">:</span>K,<span class="dt">each =</span> <span class="kw">round</span>(n<span class="op">/</span>K))[<span class="dv">1</span><span class="op">:</span>n])</a>
<a class="sourceLine" id="cb314-10" title="10">  <span class="co"># Now fit the model to each combination of K-1 chunks, and predict</span></a>
<a class="sourceLine" id="cb314-11" title="11">  <span class="co"># on the Kth chunk:</span></a>
<a class="sourceLine" id="cb314-12" title="12">  <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>K) {</a>
<a class="sourceLine" id="cb314-13" title="13">    ds &lt;-<span class="st"> </span><span class="kw">reduce</span>(datachunks[(<span class="dv">1</span><span class="op">:</span>K)[<span class="op">-</span>k]],bind_rows)</a>
<a class="sourceLine" id="cb314-14" title="14">    mod &lt;-<span class="st"> </span><span class="kw">lm</span>(answer_speed <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(date,<span class="dt">degree =</span> p,<span class="dt">raw =</span> <span class="ot">TRUE</span>),</a>
<a class="sourceLine" id="cb314-15" title="15">              <span class="dt">data =</span> ds)</a>
<a class="sourceLine" id="cb314-16" title="16">    mspevec[k] &lt;-<span class="st"> </span><span class="kw">mean</span>( (<span class="kw">predict</span>(mod,<span class="dt">newdata =</span> datachunks[[k]]) <span class="op">-</span><span class="st"> </span>datachunks[[k]]<span class="op">$</span>answer_speed)<span class="op">^</span><span class="dv">2</span> )</a>
<a class="sourceLine" id="cb314-17" title="17">  }</a>
<a class="sourceLine" id="cb314-18" title="18">  <span class="kw">mean</span>(mspevec)</a>
<a class="sourceLine" id="cb314-19" title="19">}</a>
<a class="sourceLine" id="cb314-20" title="20"><span class="kw">do_crossval</span>(<span class="dv">1</span>)</a></code></pre></div>
<pre><code>## [1] 0.9951656</code></pre>
<div class="sourceCode" id="section-cb316"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb316-1" title="1"><span class="kw">do_crossval</span>(<span class="dv">2</span>)</a></code></pre></div>
<pre><code>## [1] 0.7489409</code></pre>
<div class="sourceCode" id="section-cb318"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb318-1" title="1"><span class="kw">do_crossval</span>(<span class="dv">10</span>)</a></code></pre></div>
<pre><code>## [1] 0.6090095</code></pre>
<div class="sourceCode" id="section-cb320"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb320-1" title="1"><span class="kw">do_crossval</span>(<span class="dv">100</span>)</a></code></pre></div>
<pre><code>## [1] 4.185759</code></pre>
<p>Woah! The fit on new data–that is, the quality of the predictions–
seems to improve as the degree of polynomial increases, but only to a certain
point. For the <span class="math inline">\(p = 100\)</span> degree polynomial, the out-of-sample MSPE is terrible.</p>
<p>The reason is because of the <strong>bias-variance tradeoff</strong>. The lower-degree models
have some <strong>bias</strong>–they don’t predict perfectly in-sample–but they have low variance,
in that <em>the inferred model parameters tend to be similar across datasets</em>.</p>
<p>Consider again the <span class="math inline">\(p = 1\)</span> and <span class="math inline">\(p = 100\)</span> models:</p>
<div class="sourceCode" id="section-cb322"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb322-1" title="1">cowplot<span class="op">::</span><span class="kw">plot_grid</span>(</a>
<a class="sourceLine" id="cb322-2" title="2">  <span class="kw">prediction_plot</span>(contactmod_<span class="dv">1</span>),</a>
<a class="sourceLine" id="cb322-3" title="3">  <span class="kw">prediction_plot</span>(contactmod_<span class="dv">100</span>),</a>
<a class="sourceLine" id="cb322-4" title="4">  <span class="dt">nrow =</span> <span class="dv">1</span></a>
<a class="sourceLine" id="cb322-5" title="5">)</a></code></pre></div>
<p><img src="book_files/figure-html/contact-4-1.png" width="672" /></p>
<p>The <span class="math inline">\(p = 1\)</span> plot (exercise: make sure you can tell which is which) doesn’t
predict very many datapoints correctly. The <span class="math inline">\(p = 100\)</span> hugs the observed data
much closer. But because of this, if we sample a new dataset…</p>
<div class="sourceCode" id="section-cb323"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb323-1" title="1"><span class="co"># Take a bootstrap sample</span></a>
<a class="sourceLine" id="cb323-2" title="2">contact_new &lt;-<span class="st"> </span>contact_std[<span class="kw">sample.int</span>(<span class="kw">nrow</span>(contact_std),<span class="dt">replace =</span> <span class="ot">TRUE</span>), ]</a>
<a class="sourceLine" id="cb323-3" title="3">contactmod_new_<span class="dv">1</span> &lt;-<span class="st">   </span><span class="kw">lm</span>(answer_speed <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(date,<span class="dt">degree =</span> <span class="dv">1</span>,<span class="dt">raw =</span> <span class="ot">TRUE</span>),<span class="dt">data =</span> contact_new)</a>
<a class="sourceLine" id="cb323-4" title="4">contactmod_new_<span class="dv">100</span> &lt;-<span class="st">   </span><span class="kw">lm</span>(answer_speed <span class="op">~</span><span class="st"> </span><span class="kw">poly</span>(date,<span class="dt">degree =</span> <span class="dv">100</span>,<span class="dt">raw =</span> <span class="ot">TRUE</span>),<span class="dt">data =</span> contact_new)</a>
<a class="sourceLine" id="cb323-5" title="5">prediction_plot_new &lt;-<span class="st"> </span><span class="cf">function</span>(mod) {</a>
<a class="sourceLine" id="cb323-6" title="6">  contact_new <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb323-7" title="7"><span class="st">    </span><span class="kw">mutate</span>(<span class="dt">pred =</span> <span class="kw">predict</span>(mod)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb323-8" title="8"><span class="st">    </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> date)) <span class="op">+</span></a>
<a class="sourceLine" id="cb323-9" title="9"><span class="st">    </span><span class="kw">theme_classic</span>() <span class="op">+</span></a>
<a class="sourceLine" id="cb323-10" title="10"><span class="st">    </span><span class="kw">geom_point</span>(<span class="kw">aes</span>(<span class="dt">y =</span> answer_speed),<span class="dt">pch =</span> <span class="dv">21</span>,<span class="dt">colour =</span> <span class="st">&quot;black&quot;</span>,<span class="dt">fill =</span> <span class="st">&quot;orange&quot;</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb323-11" title="11"><span class="st">    </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">y =</span> pred),<span class="dt">colour =</span> <span class="st">&quot;purple&quot;</span>)</a>
<a class="sourceLine" id="cb323-12" title="12">}</a>
<a class="sourceLine" id="cb323-13" title="13">cowplot<span class="op">::</span><span class="kw">plot_grid</span>(</a>
<a class="sourceLine" id="cb323-14" title="14">  <span class="kw">prediction_plot_new</span>(contactmod_new_<span class="dv">1</span>),</a>
<a class="sourceLine" id="cb323-15" title="15">  <span class="kw">prediction_plot_new</span>(contactmod_new_<span class="dv">100</span>),</a>
<a class="sourceLine" id="cb323-16" title="16">  <span class="dt">nrow =</span> <span class="dv">1</span></a>
<a class="sourceLine" id="cb323-17" title="17">)</a></code></pre></div>
<p><img src="book_files/figure-html/contact-5-1.png" width="672" /></p>
<p>The <span class="math inline">\(p = 1\)</span> model remains nearly identical, while the <span class="math inline">\(p = 100\)</span> model changes!
High variance refers to the fact that slight changes in the sample cause big changes
in the model– and this leads to bad predictions on new data.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="section-supplement-to-evans-rosenthal-section-7-2.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="section-installing-r-and-rstudio.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["book.pdf", "book.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
